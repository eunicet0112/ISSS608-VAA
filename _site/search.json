[
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "This exercise details how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#learning-outcome",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "",
    "text": "This exercise details how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#getting-started",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nFirst, write a code chunk to check, install and launch the following R packages:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\nThe code chunk below will be used to accomplish the task.\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#importing-data",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#importing-data",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "3 Importing Data",
    "text": "3 Importing Data\nIn this section, Exam_data.csv provided will be used. Using read_csv() of readr package, import Exam_data.csv into R.\nThe code chunk below read_csv() of readr package is used to import Exam_data.csv data file into R and save it as an tibble data frame called exam_data.\n\nexam_data &lt;- read_csv(\"Data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#interactive-data-visualisation---ggiraph-methods",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "4 Interactive Data Visualisation - ggiraph methods",
    "text": "4 Interactive Data Visualisation - ggiraph methods\nggiraph is an htmlwidget and a ggplot2 extension. It allows ggplot graphics to be interactive.\nInteractive is made with ggplot geometries that can understand three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\nData_id: a column of data-sets that contain an id to be associated with elements.\n\nIf it used within a shiny application, elements associated with an id (data_id) can be selected and manipulated on client and server sides. Refer to this article for more detail explanation.\n\n4.1 Tooltip effect with tooltip aesthetic\nBelow shows a typical code chunk to plot an interactive statistical graph by using ggiraph package. Notice that the code chunk consists of two parts. First, an ggplot object will be created. Next, girafe() of ggiraph will be used to create an interactive svg object.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\nNotice that two steps are involved. First, an interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph. Then, girafe() will be used to generate an svg object to be displayed on an html page.\nBy hovering the mouse pointer on an data point of interest, the student’s ID will be displayed.\n\n\n4.2 Interactivity\n\n4.2.1 Displaying multiple information on tooltip\nThe content of the tooltip can be customised by including a list object as shown in the code chunk below.\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\nThe first three lines of codes in the code chunk create a new field called tooltip. At the same time, it populates text in ID and CLASS fields into the newly created field. Next, this newly created field is used as tooltip field as shown in the code of line 7.\nBy hovering the mouse pointer on an data point of interest, the student’s ID and Class will be displayed.\n\n\n4.2.2 Customising Tooltip Style\nCode chunk below uses opts_tooltip() of ggiraph to customize tooltip rendering by add css declarations.\n\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)\n\n\n\n\n\nNotice that the background colour of the tooltip is black and the font colour is white and bold.\n\nRefer to Customizing girafe objects to learn more about how to customise ggiraph objects.\n\n\n\n4.2.3 Displaying statistics on tooltip\nCode chunk below shows an advanced way to customise tooltip. In this example, a function is used to compute 90% confident interval of the mean. The derived statistics are then displayed in the tooltip.\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"light blue\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, size = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n4.2.4 Hover effect with data_id aesthetic\nCode chunk below shows the second interactive feature of ggiraph, namely data_id.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote that the default value of the hover css is hover_css = “fill:orange;”.\n\n\n4.2.5 Styling hover effect\nIn the code chunk below, css codes are used to change the highlighting effect.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\nNote: Different from previous example, in this example the ccs customisation request are encoded directly.\n\n\n4.2.6 Combining tooltip and hover effect\nThere are time that we want to combine tooltip and hover effect on the interactive statistical graph as shown in the code chunk below.\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n)                                        \n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n4.2.7 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code chunk below shown an example of onclick.\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618)                         \n\n\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\n\n\n\n\n\n\nWarning\n\n\n\nNote that click actions must be a string column in the dataset containing valid javascript instructions.\n\n\n\n\n4.2.8 Coordinated Multiple Views with ggiraph\nIn order to build a coordinated multiple views, the following programming strategy will be used:\n\nAppropriate interactive functions of ggiraph will be used to create the multiple views.\npatchwork function of patchwork package will be used inside girafe function to create the interactive coordinated multiple views.\n\n\np1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np2 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p1 + p2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #202020;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\nNotice that when a data point of one of the dotplot is selected, the corresponding data point ID on the second data visualisation will be highlighted too.\nThe data_id aesthetic is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when mouse over a point."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#interactive-data-visualisation---plotly-methods",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "5 Interactive Data Visualisation - plotly methods",
    "text": "5 Interactive Data Visualisation - plotly methods\nPlotly’s R graphing library create interactive web graphics from ggplot2 graphs and/or a custom interface to the (MIT-licensed) JavaScript library plotly.js inspired by the grammar of graphics. Different from other plotly platform, plot.R is free and open source.\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n5.1 Creating an interactive scatter plot: plot_ly() method\nThe tabset below shows an example a basic interactive plot created by using plot_ly().\n\nPlotCode Chunk\n\n\n\n\nCode\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n5.2 Working with visual variable: plot_ly() method\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nPlotCode Chunk\n\n\n\n\nCode\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\nInteractive: click on the colour symbol at the legend\n\n\n\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n5.3 Creating an interactive scatter plot: ggplotly() method\nThe code chunk below plots an interactive scatter plot by using ggplotly().\n\nPlotCode Chunk\n\n\n\n\nCode\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\n\n\n\n\n\n\n\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\nNotice that the only extra line you need to include in the code chunk is ggplotly().\n\n\n\n\n\n5.4 Coordinated Multiple Views with plotly\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nPlotCode Chunk\n\n\n\n\nCode\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\nThing to learn from the code chunk:\n\nhighlight_key() simply creates an object of class crosstalk::SharedData.\nVisit this link to learn more about crosstalk"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#interactive-data-visualisation---crosstalk-methods",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "6 Interactive Data Visualisation - crosstalk methods",
    "text": "6 Interactive Data Visualisation - crosstalk methods\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n6.1 Interactive Data Table: DT Package\n\nA wrapper of the JavaScript Library DataTables\nData objects in R can be rendered as HTML tables using the JavaScript library ‘DataTables’ (typically via R Markdown or Shiny).\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n6.2 Linked Brushing\n\nPlotCode Chunk\n\n\n\n\nCode\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode chunk below is used to implement the coordinated brushing shown above.\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\nThings to learn from the code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#reference",
    "href": "Hands_On_Exercise/Hands_On_Ex03A/Hands_On_Ex03A.html#reference",
    "title": "3 Programming Interactive Data Visualisation with R",
    "section": "7 Reference",
    "text": "7 Reference\n\n7.1 ggigraph\nThis link provides online version of the reference guide and several useful articles. Use this link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nThis link provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n7.2 plotly for R\n\nGetting Started with Plotly in R\nA collection of plotly R graphs are available via this link.\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html",
    "href": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html",
    "title": "Programming Animated Data Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#overview",
    "title": "Programming Animated Data Graphics with R",
    "section": "",
    "text": "When telling a visually-driven data story, animated graphics tends to attract the interest of the audience and make deeper impression than static graphics. In this hands-on exercise, you will learn how to create animated data visualisation by using gganimate and plotly r packages. At the same time, you will also learn how to (i) reshape data by using tidyr package, and (ii) process, wrangle and transform data by using dplyr package.\n\n\nWhen creating animations, the plot does not actually move. Instead, many individual plots are built and then stitched together as movie frames, just like an old-school flip book or cartoon. Each frame is a different plot when conveying motion, which is built using some relevant subset of the aggregate data. The subset drives the flow of the animation when stitched back together.\n\n\n\n\nBefore we dive into the steps for creating an animated statistical graph, it’s important to understand some of the key concepts and terminology related to this type of visualization.\n\nFrame: In an animated line graph, each frame represents a different point in time or a different category. When the frame changes, the data points on the graph are updated to reflect the new data.\nAnimation Attributes: The animation attributes are the settings that control how the animation behaves. For example, you can specify the duration of each frame, the easing function used to transition between frames, and whether to start the animation from the current frame or from the beginning.\n\n\n\n\n\n\n\nTip\n\n\n\nBefore you start making animated graphs, you should first ask yourself: Does it makes sense to go through the effort? If you are conducting an exploratory data analysis, a animated graphic may not be worth the time investment. However, if you are giving a presentation, a few well-placed animated graphics can help an audience connect with your topic remarkably better than static counterparts."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#getting-started",
    "title": "Programming Animated Data Graphics with R",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 Loading the R Packages\nFirst, write a code chunk to check, install and load the following R packages:\n\nplotly, R library for plotting interactive statistical graphs.\ngganimate, an ggplot extension for creating animated statistical graphs.\ngifski converts video frames to GIF animations using pngquant’s fancy features for efficient cross-frame palettes and temporal dithering. It produces animated GIFs that use thousands of colors per frame.\ngapminder: An excerpt of the data available at Gapminder.org. We just want to use its country_colors scheme.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\n\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n2.2 Importing the Data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nWrite a code chunk to import Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"Data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nread_xls() of readxl package is used to import the Excel worksheet.\nmutate_each_() of dplyr package is used to convert all character data type into factor.\nmutate of dplyr package is used to convert data values of Year field into integer.\n\n\n\nUnfortunately, mutate_each_() was deprecated in dplyr 0.7.0. and funs() was deprecated in dplyr 0.8.0. In view of this, we will re-write the code by using mutate_at() as shown in the code chunk below.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"Data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\nInstead of using mutate_at(), across() can be used to derive the same outputs.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"Data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#animated-data-visualisation-gganimate-methods",
    "title": "Programming Animated Data Graphics with R",
    "section": "3 Animated Data Visualisation: gganimate methods",
    "text": "3 Animated Data Visualisation: gganimate methods\ngganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n3.1 Building a static population bubble plot\nIn the code chunk below, the basic ggplot2 functions are used to create a static bubble plot.\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n3.2 Building the animated bubble plot\nIn the code chunk below,\n\ntransition_time() of gganimate is used to create transition through distinct states in time (i.e. Year).\nease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#animated-data-visualisation-plotly",
    "href": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#animated-data-visualisation-plotly",
    "title": "Programming Animated Data Graphics with R",
    "section": "4 Animated Data Visualisation: plotly",
    "text": "4 Animated Data Visualisation: plotly\nIn Plotly R package, both ggplotly() and plot_ly() support key frame animations through the frame argument/aesthetic. They also support an ids argument/aesthetic to ensure smooth transitions between objects with the same id (which helps facilitate object constancy).\n\n4.1 Building an animated bubble plot: ggplotly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using ggplotly() method.\n\nPlotCode Chunk\n\n\n\n\nCode\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\nThe animated bubble plot above includes a play/pause button and a slider component for controlling the animation.\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nAppropriate ggplot2 functions are used to create a static bubble plot. The output is then saved as an R object called gg.\nggplotly() is then used to convert the R graphic object into an animated svg object.\n\n\n\n\n\n\nNotice that although show.legend = FALSE argument was used, the legend still appears on the plot. To overcome this problem, theme(legend.position='none') should be used as shown in the plot and code chunk below.\n\nPlotCode Chunk\n\n\n\n\nCode\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n4.2 Building an animated bubble plot: plot_ly() method\nIn this sub-section, you will learn how to create an animated bubble plot by using plot_ly() method.\n\nPlotCode Chunk\n\n\n\n\nCode\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#reference",
    "href": "Hands_On_Exercise/Hands_On_Ex03B/Hands_OnEx03B.html#reference",
    "title": "Programming Animated Data Graphics with R",
    "section": "5 Reference",
    "text": "5 Reference\n\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "In_Class_Exercise/In_Class_Ex01/In_Class_Ex01.html",
    "href": "In_Class_Exercise/In_Class_Ex01/In_Class_Ex01.html",
    "title": "In-Class Exercise 1: Now You See It!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used:\n\ntidyverse\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In_Class_Exercise/In_Class_Ex01/In_Class_Ex01.html#loading-r-packages",
    "href": "In_Class_Exercise/In_Class_Ex01/In_Class_Ex01.html#loading-r-packages",
    "title": "In-Class Exercise 1: Now You See It!",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used:\n\ntidyverse\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)"
  },
  {
    "objectID": "In_Class_Exercise/In_Class_Ex01/In_Class_Ex01.html#importing-pisa-data",
    "href": "In_Class_Exercise/In_Class_Ex01/In_Class_Ex01.html#importing-pisa-data",
    "title": "In-Class Exercise 1: Now You See It!",
    "section": "Importing PISA Data",
    "text": "Importing PISA Data\nThe code chunk below uses read_sas() function of haven to import PISA data into the R environment.\n#| eval: false was used to prevent the code chunk from running.\n\nstu_qqq &lt;- read_sas(\"Data/cy08msp_stu_qqq.sas7bdat\")\n\nThe code chunk below uses the filter() function of dplyr to retrieve records from Singapore (SGP) only.\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")\n\nThe code chunks below uses write_rds() and read_rds() to save and read the records from Singapore in R format.\n\nwrite_rds(stu_qqq_SG, \n          \"Data/stu_qqq_SG.rds\")\n\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html",
    "href": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html",
    "title": "Be Weatherwise or Otherwise!",
    "section": "",
    "text": "According to the infographic published by the National Climate Change Secretariat (NCCS), the daily mean temperature is projected to increase by 1.4°C to 4.6°C by 2100.\n\n\n\nInfographic on Climate Change in Singapore.\n\n\nIn this exercise, visual interactivity and uncertainty visualisation methods will be employed to validate the claim presented above."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html#overview",
    "href": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html#overview",
    "title": "Be Weatherwise or Otherwise!",
    "section": "",
    "text": "According to the infographic published by the National Climate Change Secretariat (NCCS), the daily mean temperature is projected to increase by 1.4°C to 4.6°C by 2100.\n\n\n\nInfographic on Climate Change in Singapore.\n\n\nIn this exercise, visual interactivity and uncertainty visualisation methods will be employed to validate the claim presented above."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html#preliminary-set-up",
    "href": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html#preliminary-set-up",
    "title": "Be Weatherwise or Otherwise!",
    "section": "2. Preliminary Set-Up",
    "text": "2. Preliminary Set-Up\n\n2.1 Loading Relevant R Packages\nThe following code chunk uses the pacman::p_load() function to install and load the relevant R packages:\n\npacman::p_load(ggplot2, dplyr, tidyverse,\n               ggstatsplot, forecast, plotly, hrbrthemes, \n               lubridate)\n\n\n\n2.2 Importing Historical Weather Dataset\nThe historical daily temperature data was downloaded from the Meteorological Service Singapore website.\nFor simplicity, only the daily temperature records for the month of December across the years 1983, 1993, 2003, 2013 and 2023 collected by the weather station in Changi will be analysed.\nA Date column will be added by merging the Day, Month and Year columns and converted to Date format using as.Date() to facilitate subsequent time-series analysis.\n\n\nCode\ndata &lt;- read_csv(\"Data/December.csv\", show_col_types = FALSE)\ndata &lt;- data[,c(\"Year\", \"Month\", \"Day\", \"Mean Temperature (°C)\", \n                \"Maximum Temperature (°C)\", \"Minimum Temperature (°C)\")]\ndata &lt;- data %&gt;% rename(\"Mean Temperature\" = \"Mean Temperature (°C)\", \n                        \"Maximum Temperature\" = \"Maximum Temperature (°C)\", \n                        \"Minimum Temperature\" = \"Minimum Temperature (°C)\")\n\n# Adding date column \ndata$Date &lt;- as.Date(with(data, paste(Year, Month, Day,sep=\"-\")), \"%Y-%m-%d\")\n\n# Adding index column \ndata &lt;- data[with(data, order(Year, Day)), ]\ndata$Index &lt;- seq(from=1, to=nrow(data))\n\nhead(data, 5)\n\n\n# A tibble: 5 × 8\n   Year Month   Day `Mean Temperature` `Maximum Temperature`\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;              &lt;dbl&gt;                 &lt;dbl&gt;\n1  1983    12     1               26.4                  31  \n2  1983    12     2               24.3                  27.2\n3  1983    12     3               25.1                  30.2\n4  1983    12     4               25.2                  30.3\n5  1983    12     5               26                    29.8\n# ℹ 3 more variables: `Minimum Temperature` &lt;dbl&gt;, Date &lt;date&gt;, Index &lt;int&gt;"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html#visualisation-of-daily-temperature-patterns",
    "href": "Take_Home_Exercise/Take_Home_Ex03/Take_Home_Ex03.html#visualisation-of-daily-temperature-patterns",
    "title": "Be Weatherwise or Otherwise!",
    "section": "3. Visualisation of Daily Temperature Patterns",
    "text": "3. Visualisation of Daily Temperature Patterns\n\n3.1 Visual Statistical Analysis of Historical Daily Temperature Data\nTo ascertain whether daily temperatures have been displaying an upward trend, a One-Way ANOVA Test will be conducted with the following hypothesis:\nH0: There is no difference in mean daily temperatures across all years.\nH1: The mean daily temperatures for at least one year is different.\n\n\nCode\nggbetweenstats(data = data, \n               x = Year, y = \"Mean Temperature\", \n               type = \"p\",\n               mean.ci = TRUE, \n               pairwise.comparisons = TRUE,\n               pairwise.display = \"s\", \n               p.adjust.method = \"fdr\",\n               messages = FALSE)\n\n\n\n\n\nThe p-value is less than 0.05, indicating that there is sufficient evidence to reject the null hypothesis and conclude that the difference between the mean temperatures for each year is statistically significant. There is an overall increase from 25.41°C in 1983 to 27.30°C in 2023. However, the increase is not consistent across all years as there is is a slight dip from 26.56°C in 2003 to 26.48°C in 2013.\nFrom the statistical test and its visualisation, we are confident that mean daily temperatures displayed an upward trend from 1983 to 2023. With the historical trend in daily temperatures, we will move on to the next section to forecast the upcoming trend in mean daily temperatures till 2100.\n\n\n3.2 Forecasting Daily Temperatures\nTo predict daily temperatures till 2100, linear regression will be used for simplicity. By leveraging historical daily temperature data, the linear regression model is trained to identify trends and patterns in temperature changes over time to provide insights into potential future temperature trends.\nWhile linear regression is rather limited for long-term predictions due to its assumption of a linear relationship, it it inherently simple and offers a straightforward approach to modelling the relationship between variables. Linear regression will be performed using the lm() function from the stats package.\n\nMinimum TemperaturesMean TemperaturesMaximum Temperatures\n\n\n\n# Predicting future minimum temperatures using linear regression\nlm_min &lt;- lm(data$`Minimum Temperature` ~ data$Index)\nsummary(lm_min)\n\n\nCall:\nlm(formula = data$`Minimum Temperature` ~ data$Index)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3800 -0.3867  0.0589  0.5650  1.4349 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.204156   0.127068 182.612  &lt; 2e-16 ***\ndata$Index   0.012395   0.001413   8.772 3.21e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7872 on 153 degrees of freedom\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3303 \nF-statistic: 76.94 on 1 and 153 DF,  p-value: 3.21e-15\n\n\n\n\n\n# Predicting future mean temperatures using linear regression\nlm_mean &lt;- lm(data$`Mean Temperature` ~ data$Index)\nsummary(lm_mean)\n\n\nCall:\nlm(formula = data$`Mean Temperature` ~ data$Index)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.84043 -0.53462  0.05953  0.65061  1.71249 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 25.534512   0.144611 176.574  &lt; 2e-16 ***\ndata$Index   0.011766   0.001608   7.316 1.34e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8958 on 153 degrees of freedom\nMultiple R-squared:  0.2592,    Adjusted R-squared:  0.2543 \nF-statistic: 53.53 on 1 and 153 DF,  p-value: 1.341e-11\n\n\n\n\n\n# Predicting future maximum temperatures using linear regression\nlm_max &lt;- lm(data$`Maximum Temperature` ~ data$Index)\nsummary(lm_max)\n\n\nCall:\nlm(formula = data$`Maximum Temperature` ~ data$Index)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6310 -0.8814  0.3262  1.2489  3.2547 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 29.103301   0.289307 100.597  &lt; 2e-16 ***\ndata$Index   0.012604   0.003217   3.918 0.000134 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.792 on 153 degrees of freedom\nMultiple R-squared:  0.09117,   Adjusted R-squared:  0.08523 \nF-statistic: 15.35 on 1 and 153 DF,  p-value: 0.0001344\n\n\n\n\n\n\n\n3.3 Interactive Visualisation of Daily Mean Temperature\n\n\nCode for preparing data for visualisation\n# Calculate mean temperature by year\nmean_temp &lt;- data %&gt;% \n  group_by(Year) %&gt;%\n  summarise(mean_temp = mean(`Mean Temperature`)) %&gt;%\n  slice(rep(1:n()))\ndata &lt;- merge(mean_temp, data, by=\"Year\")\n\n# Creating dataframe for mean labels\nmean_label &lt;- data.frame(Day=rep(20, nrow(data)), \n                         `Mean Temperature`=c(25.00, 26.15, 26.20, 26.15, 27.00),\n                         Year=c(1983, 1993, 2003, 2013, 2023), \n                         label=c(\"Mean=25.41°C\", \"Mean=26.52°C\", \"Mean=26.56°C\", \n                                 \"Mean=26.48°C\", \"Mean=27.30°C\"))\n\n# Creating column for projected daily temperatures\ndata$`Projected Mean` &lt;-  0.011766*data$Index + 25.534512\n  \n# Creating dataframe for forecasted mean, minimum and maximum \nprojections &lt;- data.frame(\"Index\" = seq(156, 403)) # To project to 2100\nprojections$Day &lt;- rep(seq(1,31), 8)\nprojections$Month &lt;- 12\nprojections$Year &lt;- rep(c(2033, 2043, 2053, 2063, 2073, 2083, 2093, 2103), each=31)\nprojections$`Projected Minimum` &lt;- 0.012395*(projections$Index) + 23.204156\nprojections$`Projected Mean` &lt;- 0.011766*(projections$Index) + 25.534512\nprojections$`Projected Maximum` &lt;- 0.012604*(projections$Index) + 29.103301\nprojections$Date &lt;- as.Date(with(projections, paste(Year, Month, Day,sep=\"-\")), \"%Y-%m-%d\")\ndata &lt;- bind_rows(data, projections)\n\n# Creating dataframe for annotation\nannotate &lt;- data.frame(value = numeric())\nannotate &lt;- rbind(annotate, \n                  list(value=filter(data, Date == '2103-12-31')$`Projected Minimum`), \n                  list(value=filter(data, Date == '2103-12-31')$`Projected Mean`), \n                  list(value=filter(data, Date == '2103-12-31')$`Projected Maximum`))\nrownames(annotate) &lt;- c(\"Forecasted Minimum Temperature\", \"Forecasted Mean Temperature\", \n                        \"Forecasted Maximum Temperature\")\n\n\nThe interactive plot below has a few notable features:\n\nVisualisation of Uncertainty: The range of possible daily temperatures was plotted as the grey shaded area around the line graph for mean daily temperature. The minimum band represents the lower bound of uncertainty and is indicated by the minimum daily temperatures while the maximum band represents the upper bound of the uncertainty and is indicated by the maximum daily temperatures. By visualising uncertainty this way, viewers can quickly grasp the range of potential daily temperature values to understand the level of confidence or uncertainty associated with the daily temperature data presented.\nMean Temperature Line: The mean daily temperature (horizontal dark blue line) is plotted to show the average Mean Daily Temperature for each year, with the exact average value annotated in dark blue. This allows viewers to accurately identify the mean daily temperatures across all years to understand the overall change in daily temperatures over the years.\nLinear Regression Line with Uncertainty Visualisation: The forecasted daily temperatures till 2100 developed using linear regression was plotted as the red line. The uncertainty of the forecast was estimated using the linear regression projection of the daily minimum and maximum temperatures and was plotted as the red shaded area around the linear regression line. The minimum, mean and maximum temperatures for 31/12/2103 was annotated in red to aid in the validation of the claim of “daily mean temperature is projected to increase by 1.4°C to 4.6°C by 2100”.\nInteractivity: The plot is interactive as it allows viewers to zoom in on specific time periods of interests using the slider at the bottom of the plot.\n\n\n\n\n\n\n\nDesign Considerations\n\n\n\n\nIf the data is plotted as is, there would be large gaps in the line plot due to missing data in the months of January to November. This will compress the actual data points in December, causing the plot to be illegible. To avoid this, an uninterrupted daily temperature history was plotted by hiding missing time periods using the rangebreaks attribute. Other alternatives such as a stacked line plot (one line for each year) were considered but it resulted in a cluttered plot that obscured the overall trend of increasing temperatures over the years.\nThe uncertainty visualisation was represented using the minimum and maximum bands. The fill was set to “tonexty” to create an area plot with interior filling, with the Maximum Daily Temperature and Minimum Daily Temperature being set as the upper and lower bound respectively. To overlay the scatter plot over the uncertainty ribbon, the add_trace() function was used.\nThe line plot has to be inserted after the uncertainty visualisation plot so that it will not be buried under the confidence interval band, thus allowing for clearer visibility.\nThe mean lines for each year have to be plotted individually using add_trace() to ensure that the mean lines are not connected from one year to the next.\nA slider was inserted at the bottom of the plot using rangeslider to allow users to zoom in on specific time periods of interest by adjusting the range on the slider.\nTick labels were set for the x-axis using the ticktext and tickvals argument in the layout attribute to ensure that every start of the year was labelled for increased clarity.\nThe hover information was disabled for the mean lines by setting the hoverinfo='skip'. The hover information was unnecessary since it is a horizontal line and only cluttered the interactivity features.\nThe hover information was customised using the hoverinfo and text arguments.\nAnnotations were added for the forecasted temperature on 31/12/2103 in order to validate the claim mentioned above. Annotations were added using the add_anotations() function.\n\n\n\n\n\nCode for Visualising Historical Daily Temperatures\nfig &lt;- \n  \n  # Setting up uncertainty interval using daily minimum and maximum temperatures\n  plot_ly(data=filter(data, Year &lt;= '2023'), \n          x=~Date, y=~`Minimum Temperature`, \n          type='scatter', mode='lines', \n          # Setting line to transparent\n          line=list(color='rgba(211, 211, 211, 0)')) %&gt;% \n  add_trace(y=~`Maximum Temperature`, \n            type='scatter', mode='lines', \n            line=list(color='rgba(211, 211, 211, 0)'),\n            # Setting fill to opacity of 0.3\n            fill='tonexty', fillcolor='rgba(211, 211, 211, 0.3)') %&gt;% \n  \n  # Setting up line plot for historical daily mean temperature\n  add_trace(x=~Date, y=~`Mean Temperature`, \n            type='scatter', mode='lines+markers', \n            marker=list(color=\"#88CDF6\", size=5), \n            line=list(color=\"#88CDF6\", width=1.5),\n            hoverinfo='text', \n            text= ~paste('&lt;/br&gt; Date: ', Date,\n                         '&lt;/br&gt; Mean Temperature: ', `Mean Temperature`, \"°C\")) \n\n\nfig &lt;- fig %&gt;%\n  # Setting up mean line\n  add_trace(data=filter(data, Year == '1983'),\n          x=~Date, y=~mean_temp, \n          type='scatter', mode='lines', \n          line=list(color='#015C92', width=1.5),\n          # Disabling hover information\n          hoverinfo='skip') %&gt;% \n  \n  # Adding annotation for mean line \n  layout(annotations=list(x=\"1983-12-20\", y=25.00, text=\"Mean=25.41°C\", showarrow=FALSE, \n                           font=list(color=\"#015C92\", size=9))) %&gt;% \n  \n  add_trace(data=filter(data, Year == '1993'),\n          x=~Date, y=~mean_temp, \n          type='scatter', mode='lines', \n          line=list(color='#015C92', width=1.5), \n          hoverinfo='skip') %&gt;% \n  layout(annotations=list(x=\"1993-12-20\", y=26.15, text=\"Mean=26.52°C\", showarrow=FALSE, \n                           font=list(color=\"#015C92\", size=9))) %&gt;% \n  \n  add_trace(data=filter(data, Year == '2003'),\n          x=~Date, y=~mean_temp, \n          type='scatter', mode='lines', \n          line=list(color='#015C92', width=1.5), \n          hoverinfo='skip') %&gt;% \n  layout(annotations=list(x=\"2003-12-20\", y=26.20, text=\"Mean=26.56°C\", showarrow=FALSE, \n                           font=list(color=\"#015C92\", size=9))) %&gt;% \n  \n  add_trace(data=filter(data, Year == '2013'),\n          x=~Date, y=~mean_temp, \n          type='scatter', mode='lines', \n          line=list(color='#015C92', width=1.5), \n          hoverinfo='skip') %&gt;% \n  layout(annotations=list(x=\"2013-12-20\", y=26.15, text=\"Mean=26.48°C\", showarrow=FALSE, \n                           font=list(color=\"#015C92\", size=9))) %&gt;% \n  \n  add_trace(data=filter(data, Year == '2023'),\n          x=~Date, y=~mean_temp, \n          type='scatter', mode='lines', \n          line=list(color='#015C92', width=1.5), \n          hoverinfo='skip') %&gt;% \n  layout(annotations=list(x=\"2023-12-20\", y=27.00, text=\"Mean=27.30°C\", showarrow=FALSE, \n                           font=list(color=\"#015C92\", size=9))) \n\n\n\n\nCode for Visualising Forecasted Daily Temperatures\nfig &lt;- fig %&gt;% \n  \n  # Setting up linear regression line\n  add_trace(data=filter(data, Year &gt;= '2033'), \n            x=~Date, y=~`Projected Mean`, \n            type='scatter', mode='lines', \n            line=list(color='#8F0000', width=1.5), \n            # Formatting hover text information\n            hoverinfo='text', \n            text = ~paste('&lt;/br&gt; Date: ', Date,\n                          '&lt;/br&gt; Forecasted Mean Temperature', format(round(`Projected Mean`,2)), '°C')) %&gt;% \n  \n  # Setting up uncertainty interval using forecasted minimum and maximum temperatures\n  add_trace(data=filter(data, Year &gt;= '2033'), \n            x=~Date, y=~`Projected Minimum`, \n            type='scatter', mode='lines', \n            line=list(color='rgba(241, 169, 160, 0'),\n            hoverinfo='text', \n            text = ~paste('&lt;/br&gt; Date: ', Date,\n                          '&lt;/br&gt; Forecasted Minimum Temperature', format(round(`Projected Minimum`,2)), '°C')) %&gt;% \n  add_trace(y=~`Projected Maximum`, \n            type='scatter', mode='lines', \n            line=list(color='rgba(241, 169, 160, 0)'), \n            fill='tonexty', fillcolor='rgba(241, 169, 160, 0.3)', \n            hoverinfo='text', \n            text = ~paste('&lt;/br&gt; Date: ', Date,\n                          '&lt;/br&gt; Forecasted Maximum Temperature', format(round(`Projected Maximum`,2)), '°C'))\n\n\n\n\nCode for Formatting Layout of Visualisation\nfig %&gt;% \n  layout(showlegend=FALSE, # Switching off legend\n         xaxis=list(\n           # Specifying range of dates for plot\n           range=c(\"1983-12-01\", \"2103-12-31\"), \n                    # Hiding time periods with missing data\n                    rangebreaks=list(\n                      list(bounds=list(\"1984-01-01\", \"1993-12-01\")), \n                      list(bounds=list(\"1994-01-01\", \"2003-12-01\")), \n                      list(bounds=list(\"2004-01-01\", \"2013-12-01\")), \n                      list(bounds=list(\"2014-01-01\", \"2023-12-01\")), \n                      list(bounds=list(\"2024-01-01\", \"2033-12-01\")),\n                      list(bounds=list(\"2034-01-01\", \"2043-12-01\")),\n                      list(bounds=list(\"2044-01-01\", \"2053-12-01\")),\n                      list(bounds=list(\"2054-01-01\", \"2063-12-01\")),\n                      list(bounds=list(\"2064-01-01\", \"2073-12-01\")),\n                      list(bounds=list(\"2074-01-01\", \"2083-12-01\")),\n                      list(bounds=list(\"2084-01-01\", \"2093-12-01\")),\n                      list(bounds=list(\"2094-01-01\", \"2103-12-01\"))),  \n                    \n                    # Inserting range slider for time period selection\n                    rangeslider=list(type=Date,\n                                     range=c(\"1983-12-01\", \"2103-12-31\")),\n           \n           # Specifying tick labels\n           ticktext=list(\"1983\", \"1993\", \"2003\", \"2013\", \"2023\", \"2033\", \"2043\", \n                         \"2053\", \"2063\", \"2073\", \"2083\", \"2093\", \"2103\"), \n           tickvals=list(\"1983-12-01\", \"1993-12-01\", \"2003-12-01\", \"2013-12-01\", \n                       \"2023-12-01\", \"2033-12-01\", \"2043-12-01\", \"2053-12-01\", \n                       \"2063-12-01\", \"2073-12-01\", \"2083-12-01\", \"2093-12-01\", \"2103-12-01\"),\n           \n           # Labelling x-axis\n           title=\"Date\"), \n         # Labelling y-axis\n         yaxis=list(title=\"Daily Temperature (°C)\"), \n         # Labelling plot\n         title=\"Daily temperatures for December in Changi are expected to \\nincrease through to 2100\") %&gt;% \n  \n  add_annotations(x=\"2103-12-31\", y=annotate$value, \n                  text= ~paste(rownames(annotate), \": \", format(round(annotate$value,2)), \"°C\", sep=\"\"),\n                  xref=\"x\", yref=\"y\", \n                  xanchor=\"right\", ay=c(10, -10, 10),\n                  showarrow=TRUE, arrowhead=4, arrowsize=0.5, arrowcolor=\"#8F0000\",\n                  font = list(size=10, color=\"#8F0000\"))\n\n\n\n\n\n\nFrom the visualisation, the following can be inferred:\n\nSeasonality in daily mean temperatures was observed across all years from 1983 to 2023.\nDaily mean temperatures displayed an overall increase from 25.41°C in 1983 to 27.30°C in 2023.\nBased on forecasts from the linear regression model, daily minimum, mean and maximum temperatures are expected to reach 28.20°C, 30.28°C and 34.18°C by 31 December 2103. The claim of “daily mean temperature is projected to increase by 1.4°C to 4.6°C by 2100” is thus supported. Daily mean temperatures are projected to increase from 27.30°C in 2023 to 30.28°C in 2103. The 2.98°C increase thus falls within the range projected by NCCS and supports its claim. If the upper band of the projection is used, daily temperature is projected to increase from 27.30°C in 2023 to 34.18°C in 2103. The 6.88°C increase falls outside the range projected by NCCS and paints a grimmer image for climate change in Singapore."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html",
    "title": "Is Every School a Good School?",
    "section": "",
    "text": "Is every school a good school? According to OECD education director Andreas Schleicher, Singapore “managed to achieve excellence without wide differences between children from wealthy and disadvantaged families”. This spells good news for Singapore as its education system strives towards every school being a good school. Yet, public sentiments differs starkly from this. There is a strong belief that disparities continue to exist between elite and neighbourhood schools, between students from higher and lower socioeconomic status, and between students from immigrant and non-immigrant families.\nIn this short exercise, we will study the 2022 Programme for International Student Assessment (PISA) data to reveal the following:\n\nthe distribution of Singapore students’ performance in Mathematics, Reading and Science; and\nthe relationship between the aforementioned performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#loading-relevant-r-packages",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#loading-relevant-r-packages",
    "title": "Is Every School a Good School?",
    "section": "2.1 Loading Relevant R Packages",
    "text": "2.1 Loading Relevant R Packages\nThe following code chunk uses the pacman::p_load() function to install and load the relevant R packages:\n\npacman::p_load(haven, tidyverse, ggplot2, e1071, dplyr, \n               forcats, rstatix,kableExtra, ggstatsplot, \n               hrbrthemes, tidyr, viridis, ggridges, nortest,\n               tm, proustr, VennDiagram)"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#importing-pisa-dataset",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#importing-pisa-dataset",
    "title": "Is Every School a Good School?",
    "section": "2.2 Importing PISA Dataset",
    "text": "2.2 Importing PISA Dataset\nThe dataset provided for this exercise is extremely large with over 600,000 records from students globally. For the purpose of our task, we will filter the dataset to only include records of students from Singapore (with country code “SGP”). The resultant dataset stu_sg has 6606 records.\nThe data dictionary of all 1279 variables can be found in the PISA 2022 Database Codebook.\n\n\nCode\nstu_qqq &lt;- read_sas(\"Data/cy08msp_stu_qqq.sas7bdat\")\nstu_sg &lt;- stu_qqq %&gt;% filter(CNT == \"SGP\")\nwrite_rds(stu_sg, \"Data/stu_SG.rds\")\n\n\n\nstu_sg &lt;- read_rds(\"data/stu_sg.rds\")"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#distribution-of-performance-in-mathematics",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#distribution-of-performance-in-mathematics",
    "title": "Is Every School a Good School?",
    "section": "3.1 Distribution of Performance in Mathematics",
    "text": "3.1 Distribution of Performance in Mathematics\nThe code chunk below defines the performance bands based on the lower and upper bound for scores as stipulated by PISA.\n\nrects_math &lt;- data.frame(ystart = c(46, 420.07, 482.38, 544.68, 606.99, 669.30), \n                    yend = c(420.07, 482.38, 544.68, 606.99, 669.30, 950),\n                    Levels = c(\"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level 6\"))\n\n\n\n\n\n\n\nDesign Considerations for Violin Plot\n\n\n\n\nPlotting of the performance bands using geom_rect() has to precede all other plots to ensure that it is set as the background of the plot.\nBy default, geom_violin() sets trim=TRUE and the tails of the violins are trimmed. It was thus set to FALSE to ensure the entire range of performance scores is included in the plot.\nThe five-number summary statistics (minimum, first quartile, median, third quartile and maximum in black font) and the mean (in blue font) are displayed on the plot for greater clarity.\n\n\n\n\n\nCode\nggplot(data=stu_sg, aes(x=CNTRYID, y=as.numeric(PV5MATH))) + \n  \n  # Adding background rectangle shading to mark out performance bands\n  geom_rect(data=rects_math,\n            aes(ymin=as.numeric(ystart), ymax=as.numeric(yend), xmin=-Inf, xmax=Inf, fill=Levels),\n            inherit.aes=FALSE, alpha=0.15) +\n  \n  \n  # Adding axis labels, title and subtitle\n  labs(x=\"\", y=\"PV5 Scores for Mathematics\",\n       title=\"Mathematics Scores of Students\", \n       subtitle=\"\\n Mean score labelled in blue. Minimum, quantiles and maximum labelled in black.\") +\n  \n  # Adding violin plot\n  geom_violin(trim=FALSE, color=\"red\", alpha=0) + \n  \n  # Adding boxplot\n  geom_boxplot(width=0.1, color=\"black\", alpha=0) + \n  \n  # Adding summary statistic annotations\n  stat_summary(geom=\"text\", fun.y=quantile, \n               aes(label=sprintf(\"%1.1f\", ..y..)),\n               position=position_nudge(x=0.15), size=3) +\n  \n  # Adding mean annotation\n  stat_summary(geom=\"point\", fun.y=\"mean\", color=\"blue\", fill=\"blue\", size=2, shape=21) +\n  stat_summary(geom=\"text\", fun.y=\"mean\", color=\"blue\", \n               aes(label=sprintf(\"%1.1f\", ..y..)),\n               position=position_nudge(x=0.09), size=3) + \n\n  # Setting x-axis limits as CNTRYID was used as x-variable arbitrarily - all records have CTRYID of 702\n  scale_x_continuous(limits=c(701.5, 702.5)) +\n  \n  coord_flip() + \n  theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())\n\n\n\n\n\n\n\n\n\n\n\nInsights on Mathematics Performance\n\n\n\n\nThe mean score for Mathematics is 573.4 while the median score is 580.0. Both scores fall within Level 4 on the Proficiency Scale, indicating that the average student in Singapore are moderately proficient in Mathematics.\nThe distribution displays a slight left skew (skewness = -0.246 using skewness()). With the peak or mode of the distribution lying towards the right, there is a larger number of students with performance scores that are closer to the higher end.\nThe lower bound of Level 5 proficiency of 606.99 lies between the median (580.0) and the upper quartile (647.3). This indicates that a rather substantial portion (albeit less than 50%) of students are top performers in Mathematics."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#distribution-of-performance-in-science",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#distribution-of-performance-in-science",
    "title": "Is Every School a Good School?",
    "section": "3.2 Distribution of Performance in Science",
    "text": "3.2 Distribution of Performance in Science\nUsing the same approach as Section 3.1, the violin plot was generated for Science scores.\n\n\nCode\nrects_sci &lt;- data.frame(ystart = c(39, 409.54, 484.14, 558.73, 633.33, 707.93), \n                    yend = c(409.54, 484.14, 558.73, 633.33, 707.93, 924),\n                    Levels = c(\"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level 6\"))\n\nggplot(data=stu_sg, aes(x=CNTRYID, y=as.numeric(PV5SCIE))) + \n  \n  # Adding background rectangle shading to mark out performance bands\n  geom_rect(data=rects_sci,\n            aes(ymin=as.numeric(ystart), ymax=as.numeric(yend), xmin=-Inf, xmax=Inf, fill=Levels),\n            inherit.aes=FALSE, alpha=0.15) +\n  \n  \n  # Adding axis labels, title and subtitle\n  labs(x=\"\", y=\"PV5 Scores for Science\",\n       title=\"Science Scores of Students\", \n       subtitle=\"\\n Mean score labelled in blue. Minimum, quantiles and maximum labelled in black.\") +\n  \n  # Adding violin plot\n  geom_violin(trim=FALSE, color=\"red\", alpha=0) + \n  \n  # Adding boxplot\n  geom_boxplot(width=0.1, color=\"black\", alpha=0) + \n  \n  # Adding summary statistic annotations\n  stat_summary(geom=\"text\", fun.y=quantile, \n               aes(label=sprintf(\"%1.1f\", ..y..)),\n               position=position_nudge(x=0.15), size=3) +\n  \n  # Adding mean annotation\n  stat_summary(geom=\"point\", fun.y=\"mean\", color=\"blue\", fill=\"blue\", size=2, shape=21) +\n  stat_summary(geom=\"text\", fun.y=\"mean\", color=\"blue\", \n               aes(label=sprintf(\"%1.1f\", ..y..)),\n               position=position_nudge(x=0.09), size=3) + \n\n  # Setting x-axis limits as CNTRYID was used as x-variable arbitrarily - all records have CTRYID of 702\n  scale_x_continuous(limits=c(701.5, 702.5)) +\n  \n  coord_flip() + \n  theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())\n\n\n\n\n\n\n\n\n\n\n\nInsights on Science Performance\n\n\n\n\nThe mean score for Science is 561.5 while the median score is 569.7. Both scores fall within Level 4 on the Proficiency Scale, indicating that the average student in Singapore are moderately proficient in Science.\nThe distribution displays a slight left skew (skewness = -0.297 using skewness()). With the peak or mode of the distribution lying towards the right, there is a larger number of students with performance scores that are closer to the higher end.\nAround 25% of students attained scores more than or equal to 632.1. With the score point range starting from 633.33 for Level 5, slightly less than 25% of Singapore students are top performers in Science."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#distribution-of-performance-in-reading",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#distribution-of-performance-in-reading",
    "title": "Is Every School a Good School?",
    "section": "3.3 Distribution of Performance in Reading",
    "text": "3.3 Distribution of Performance in Reading\nUsing the same approach as Section 3.1, the violin plot was generated for Reading scores.\n\n\nCode\nrects_read &lt;- data.frame(ystart = c(0, 407.47, 480.18, 552.89, 625.61, 698.32), \n                    yend = c(407.47, 480.18, 552.89, 625.61, 698.32, 938),\n                    Levels = c(\"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level 6\"))\n\nggplot(data=stu_sg, aes(x=CNTRYID, y=as.numeric(PV5READ))) + \n  \n  # Adding background rectangle shading to mark out performance bands\n  geom_rect(data=rects_sci,\n            aes(ymin=as.numeric(ystart), ymax=as.numeric(yend), xmin=-Inf, xmax=Inf, fill=Levels),\n            inherit.aes=FALSE, alpha=0.15) +\n  \n  \n  # Adding axis labels, title and subtitle\n  labs(x=\"\", y=\"PV5 Scores for Reading\",\n       title=\"Science Scores of Reading\", \n       subtitle=\"\\n Mean score labelled in blue. Minimum, quantiles and maximum labelled in black.\") +\n  \n  # Adding violin plot\n  geom_violin(trim=FALSE, color=\"red\", alpha=0) + \n  \n  # Adding boxplot\n  geom_boxplot(width=0.1, color=\"black\", alpha=0) + \n  \n  # Adding summary statistic annotations\n  stat_summary(geom=\"text\", fun.y=quantile, \n               aes(label=sprintf(\"%1.1f\", ..y..)),\n               position=position_nudge(x=0.15), size=3) +\n  \n  # Adding mean annotation\n  stat_summary(geom=\"point\", fun.y=\"mean\", color=\"blue\", fill=\"blue\", size=2, shape=21) +\n  stat_summary(geom=\"text\", fun.y=\"mean\", color=\"blue\", \n               aes(label=sprintf(\"%1.1f\", ..y..)),\n               position=position_nudge(x=0.09), size=3) + \n\n  # Setting x-axis limits as CNTRYID was used as x-variable arbitrarily - all records have CTRYID of 702\n  scale_x_continuous(limits=c(701.5, 702.5)) +\n  \n  coord_flip() + \n  theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank())\n\n\n\n\n\n\n\n\n\n\n\nInsights on Reading Performance\n\n\n\n\nThe mean score for Reading is 541.9 while the median score is 551.2. Both scores fall within Level 3 on the Proficiency Scale, indicating that an average student in Singapore are moderately proficient in Reading. However, students recorded an average of Level 4 in Mathematics and Science, thus indicating that students in Singapore may be more proficient in Mathematics and Science compared to Reading\nThe distribution displays a slight left skew (skewness = -0.366 using skewness()). With the peak or mode of the distribution lying towards the right, there is a larger number of students with performance scores that are closer to the higher end.\nAround 25% of students attained scores more than or equal to 617.1. With the score point range starting from 625.61 for Level 5, slightly less than 25% of Singapore students are top performers in Reading."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#relationship-between-performance-and-schools",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#relationship-between-performance-and-schools",
    "title": "Is Every School a Good School?",
    "section": "4.1 Relationship between Performance and Schools",
    "text": "4.1 Relationship between Performance and Schools\nThe Ministry of Education (MOE) in Singapore grouped schools into nine bands with each band comprising schools with similar academic performance. The top 50 Special/Express and top 40 Normal course ranking tables were published to rank schools’ academic performances. While the school banding by academic results was abolished in 2012, informal school rankings developed by parents or interested parties continue to exist to this day. This highlights a common sentiments among parents and students - not all schools are equal and some are better than others.\nIn this section, we explore the relationship between schools and the performance of its students. Specifically, we ask if the choice of school affects the performance of students in terms of their performance in Mathematics, Science and Reading.\n\n4.1.1 Grouping Schools for Meaningful Analysis\nIn the PISA database, the schools of individual students are recorded as 8-character code CNTSCHID. There are 164 unique CNTSCHID in our stu_sg database. Such large number of school categories will clutter our visualisations and make subsequent analysis unmeaningful. For increased clarity, we will reduce the number of school categories by grouping the schools into 4 groups - Groups 1 to 4. For each subject, Group 1 will consist of schools with the lowest scores, followed by Group 2 then Group 3, with Group 4 consisting of schools with the highest scores.\n\n\n\n\n\n\nProcess for Grouping Schools\n\n\n\n\nThe group_by() function from the dplyr package was used to group the dataframe using the CNTSCHID column then finding the mean of the Plausible Value 5 (PV5) scores for each group\nThe resultant dataframe (consisting of School ID and corresponding mean PV5 scores) was sorted based on the mean PV5 scores in ascending order using the order() function.\nThe ordered list for schools is split into four equal groups, with Group 1 consisting of schools with the lowest scores and Group 4 consisting of schools with the highest scores.\n\n\n\nGroup\nRanked (in ascending order) Data Points\n\n\n\n\n1\n1 - 41\n\n\n2\n42 - 82\n\n\n3\n83 - 123\n\n\n4\n124 - 164\n\n\n\nThe schools are labelled based on the group they belong to, namely Group 1, Group 2, Group 3 and Group 4, in a new column called group.\n\n\n\n\n\nCode Block for Grouping Schools for Mathematics\n# Group based on School ID then find mean PV5 score\nmath_sch &lt;- stu_sg %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise(mean = mean(PV5MATH))\n\n# Sort based on mean PV5 score\nmath_sch &lt;- math_sch[order(math_sch$mean), ]\n\n# Label where school lies\nmath_sch$group &lt;- c(rep(\"Group 1\", 41), rep(\"Group 2\", 41), rep(\"Group 3\", 41), rep(\"Group 4\", 41))\n\n\n\n\nCode Block for Grouping Schools for Science\n# Group based on School ID then find mean PV5 score\nsci_sch &lt;- stu_sg %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise(mean = mean(PV5SCIE))\n\n# Sort based on mean PV5 score\nsci_sch &lt;- sci_sch[order(sci_sch$mean), ]\n\n# Label where school lies\nsci_sch$group &lt;- c(rep(\"Group 1\", 41), rep(\"Group 2\", 41), rep(\"Group 3\", 41), rep(\"Group 4\", 41))\n\n\n\n\nCode Block for Grouping Schools for Reading\n# Group based on School ID then find mean PV5 score\nread_sch &lt;- stu_sg %&gt;%\n  group_by(CNTSCHID) %&gt;%\n  summarise(mean = mean(PV5READ))\n\n# Sort based on mean PV5 score\nread_sch &lt;- read_sch[order(read_sch$mean), ]\n\n# Label where school lies\nread_sch$group &lt;- c(rep(\"Group 1\", 41), rep(\"Group 2\", 41), rep(\"Group 3\", 41), rep(\"Group 4\", 41))\n\n\n\n\n4.1.2 Relationship between Mathematics Scores and School\nThe relationship between Mathematics scores and the school the students attend will be visualised using a box plot. As box plots helps visualise the center and spread of data, clear comparisons of the mean performance between school groups can be made using side-by-side box plots.\n\n\n\n\n\n\nDesign Consideration for Box Plot\n\n\n\n\nThe mean value can be reflected in the plot using geom_text() and in-built summary functionality stat_summary()of ggplot2. As the dataframe math_sch has a variable called mean, stat_summary() refers to the variable instead of the mean function from the {base} package. To circumvent this issue, the fully qualified name base::mean is used instead of the base mean.\nThe default grid background can be removed for greater clarity using theme_bw().\nThe legend was removed using theme() as it was unnecessary for interpretation of the plot.\n\n\n\n\nEDA PlotNormality TestANOVA\n\n\nBased on the box plots, it appears that Mathematics scores are lowest in schools in the min group and highest in schools in the max group.\n\n\nCode\nmath_sch %&gt;%\n  \n  # Plotting boxplot of mean scores against groups\n  ggplot(aes(x=group, y=mean, fill=group)) +\n  geom_boxplot(alpha=0.5) +\n  \n  # Labelling plot and axis\n  ggtitle(\"Mean Mathematics Score Across Groups\") + \n  xlab(\"\") + \n  ylab(\"Mean PV5MATH Score\") +\n  \n  # Adding mean value for each group\n  stat_summary(fun=base::mean, geom=\"point\", shape=20, size=2, color=\"red\", fill=\"red\") +\n  stat_summary(fun=base::mean, color=\"red\", geom=\"text\", vjust=-4, size=3, \n               aes(label=paste(\"Mean:\", round(..y.., digits=1)))) + \n  \n  # Removing grid background\n  theme_bw() +\n  \n  # Removing legend\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\nTo ascertain our observation, tests for normality was first conducted on the distribution for each school group. A visual assessment via a Q-Q plot and a significance test via the Shapiro-Wilk’s test were used to ascertain whether the data show a serious deviation from normality. The Q-Q plot was plotted using the geom_qq() and geom_qq_line() functions from the ggplot library while the Shapiro-Wilk’s test was conducted using the shapiro_test() function from the rstatix library.\n\n\nCode\nmath_sch %&gt;%\n  ggplot(aes(sample=mean)) +\n  geom_qq() + geom_qq_line() + \n  facet_wrap(~group, scales=\"free_y\", nrow=1) +\n  labs(title=\"Normality Assumption Test\",\n       x=\"\", y=\"\") + \n  theme_bw() + \n  theme(axis.text.x=element_blank(), \n        axis.text.y=element_blank(), \n        panel.grid.major=element_blank(),\n        panel.grid.minor=element_blank(),\n        axis.ticks=element_blank())\n\n\n\n\n\n\n\nCode\nmath_sch %&gt;%\n  group_by(group) %&gt;%\n  shapiro_test(mean) %&gt;% \n  kbl() %&gt;%\n  kable_styling(full_width=FALSE, bootstrap_options = \"responsive\")\n\n\n\n\n\n\ngroup\nvariable\nstatistic\np\n\n\n\n\nGroup 1\nmean\n0.6774551\n0.0000000\n\n\nGroup 2\nmean\n0.9675082\n0.2851858\n\n\nGroup 3\nmean\n0.9253653\n0.0101652\n\n\nGroup 4\nmean\n0.9665143\n0.2640175\n\n\n\n\n\n\n\n\nSince the p-values for Groups 1 and 3 are less than 0.05, Mathematics scores do not follow a normal distribution across all groups. A non-parametric ANOVA test will thus be used to compare the mean Mathematics score.\n\n\nA non-parametric Kruskal-Wallis One-Way ANOVA test was used to compare the mean Mathematics scores across groups, with the following hypothesis:\nH0: The mean Mathematics scores is the same for all school groups.\nH1: The mean Mathematics scores is not the same for all school groups.\nThe Kruskal-Wallist and post-hoc tests was conducted using the ggbetweenstats() function from the ggstatsplot package.\n\n\nCode\nggbetweenstats(\n  data=math_sch,x=group, y=mean,\n  type=\"nonparametric\",plot.type=\"box\",\n  pairwise.comparisons=TRUE, pairwise.display=\"significant\", \n  centrality.plotting=FALSE, bf.message=FALSE)\n\n\n\n\n\nThe p-values of the Kruskal-Wallis test and all pairwise tests are less than 0.05. There is thus sufficient evidence to reject H0 and conclude that the mean Mathematics score is significantly different for each group.\n\n\n\n\n\n\n\n\n\nInsights on Relationship between Schools and Mathematics Performance\n\n\n\n\nMathematics scores are lowest in Group 1 schools and highest in Group 4 schools. The difference in mean scores between the two groups is 144.6.\nMathematics scores increases in the following order: Group 1, Group 2, Group 3, Group 4.\n\n\n\n\n\n4.1.3 Relationship between Science Scores and School\n\nEDA PlotNormalityANOVA\n\n\nBased on the bar chart, it appears that Science scores are lowest in schools in the min group and highest in schools in the max group.\n\n\nCode\nsci_sch %&gt;%\n  \n  # Plotting boxplot of mean scores against groups\n  ggplot(aes(x=group, y=mean, fill=group)) +\n  geom_boxplot(alpha=0.5) +\n  \n  # Labelling plot and axis\n  ggtitle(\"Mean Science Score Across Groups\") + \n  xlab(\"\") + \n  ylab(\"Mean PV5SCIE Score\") +\n  \n  # Adding mean value for each group\n  stat_summary(fun=base::mean, geom=\"point\", shape=20, size=2, color=\"red\", fill=\"red\") +\n  stat_summary(fun=base::mean, color=\"red\", geom=\"text\", vjust=-3, size=3, \n               aes(label=paste(\"Mean:\", round(..y.., digits=1)))) + \n  \n  # Removing grid background\n  theme_bw() +\n  \n  # Removing legend\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nCode\nsci_sch %&gt;%\n  ggplot(aes(sample=mean)) +\n  geom_qq() + geom_qq_line() + \n  facet_wrap(~group, scales=\"free_y\", nrow=1) +\n  labs(title=\"Normality Assumption Test\",\n       x=\"\", y=\"\") + \n  theme_bw() + \n  theme(axis.text.x=element_blank(), \n        axis.text.y=element_blank(), \n        panel.grid.major=element_blank(),\n        panel.grid.minor=element_blank(),\n        axis.ticks=element_blank())\n\n\n\n\n\n\n\nCode\nsci_sch %&gt;%\n  group_by(group) %&gt;%\n  shapiro_test(mean) %&gt;% \n  kbl() %&gt;%\n  kable_styling(full_width=FALSE, bootstrap_options = \"responsive\")\n\n\n\n\n\n\ngroup\nvariable\nstatistic\np\n\n\n\n\nGroup 1\nmean\n0.7447895\n0.0000005\n\n\nGroup 2\nmean\n0.9431588\n0.0403869\n\n\nGroup 3\nmean\n0.9116637\n0.0037318\n\n\nGroup 4\nmean\n0.9505109\n0.0729322\n\n\n\n\n\n\n\n\nSince the p-values for Groups 1, 2 and 3 are less than 0.05, Science scores do not follow a normal distribution across all groups. A non-parametric ANOVA test will thus be used to compare the mean Science score.\n\n\nA non-parametric Kruskal-Wallis One-Way ANOVA test was used to compare the mean Science scores across groups, with the following hypothesis:\nH0: The mean Science scores is the same for all school groups.\nH1: The mean Science scores is not the same for all school groups.\nThe Kruskal-Wallist and post-hoc tests was conducted using the ggbetweenstats() function from the ggstatsplot package.\n\n\nCode\nggbetweenstats(\n  data=sci_sch,x=group, y=mean,\n  type=\"nonparametric\",plot.type=\"box\",\n  pairwise.comparisons=TRUE, pairwise.display=\"significant\", \n  centrality.plotting=FALSE, bf.message=FALSE)\n\n\n\n\n\nThe p-values of the Kruskal-Wallis test and all pairwise tests are less than 0.05. There is thus sufficient evidence to reject H0 and conclude that the mean Science score is significantly different for each group.\n\n\n\n\n\n\n\n\n\nInsights on Relationship between Schools and Science Performance\n\n\n\n\nScience scores are lowest in Group 1 schools and highest in Group 4 schools. The difference in mean scores between the two groups is 138.9.\nScience scores increases in the following order: Group 1, Group 2, Group 3, Group 4.\n\n\n\n\n\n4.1.4 Relationship between Reading Scores and Schools\n\nEDA PlotNormalityANOVA\n\n\nBased on the bar chart, it appears that Reading scores are lowest in schools in the min group and highest in schools in the max group.\n\n\nCode\nread_sch %&gt;%\n  \n  # Plotting boxplot of mean scores against groups\n  ggplot(aes(x=group, y=mean, fill=group)) +\n  geom_boxplot(alpha=0.5) +\n  \n  # Labelling plot and axis\n  ggtitle(\"Mean Reading Score Across Groups\") + \n  xlab(\"\") + \n  ylab(\"Mean PV5READ Score\") +\n  \n  # Adding mean value for each group\n  stat_summary(fun=base::mean, geom=\"point\", shape=20, size=2, color=\"red\", fill=\"red\") +\n  stat_summary(fun=base::mean, color=\"red\", geom=\"text\", vjust=-3, size=3, \n               aes(label=paste(\"Mean:\", round(..y.., digits=1)))) + \n  \n  # Removing grid background\n  theme_bw() +\n  \n  # Removing legend\n  theme(legend.position=\"none\")\n\n\n\n\n\n\n\n\n\nCode\nread_sch %&gt;%\n  ggplot(aes(sample=mean)) +\n  geom_qq() + geom_qq_line() + \n  facet_wrap(~group, scales=\"free_y\", nrow=1) +\n  labs(title=\"Normality Assumption Test\",\n       x=\"\", y=\"\") + \n  theme_bw() + \n  theme(axis.text.x=element_blank(), \n        axis.text.y=element_blank(), \n        panel.grid.major=element_blank(),\n        panel.grid.minor=element_blank(),\n        axis.ticks=element_blank())\n\n\n\n\n\n\n\nCode\nread_sch %&gt;%\n  group_by(group) %&gt;%\n  shapiro_test(mean) %&gt;% \n  kbl() %&gt;%\n  kable_styling(full_width=FALSE, bootstrap_options = \"responsive\")\n\n\n\n\n\n\ngroup\nvariable\nstatistic\np\n\n\n\n\nGroup 1\nmean\n0.6930397\n0.0000001\n\n\nGroup 2\nmean\n0.9266733\n0.0112172\n\n\nGroup 3\nmean\n0.9316844\n0.0164318\n\n\nGroup 4\nmean\n0.9591611\n0.1469983\n\n\n\n\n\n\n\n\nSince the p-values for Groups 1, 2 and 3 are less than 0.05, Reading scores do not follow a normal distribution across all groups. A non-parametric ANOVA test will thus be used to compare the mean Reading score.\n\n\nA non-parametric Kruskal-Wallis One-Way ANOVA test was used to compare the mean Reading scores across groups, with the following hypothesis:\nH0: The mean Reading scores is the same for all school groups.\nH1: The mean Reading scores is not the same for all school groups.\nThe Kruskal-Wallist and post-hoc tests was conducted using the ggbetweenstats() function from the ggstatsplot package.\n\n\nCode\nggbetweenstats(\n  data=read_sch,x=group, y=mean,\n  type=\"nonparametric\",plot.type=\"box\",\n  pairwise.comparisons=TRUE, pairwise.display=\"significant\", \n  centrality.plotting=FALSE, bf.message=FALSE)\n\n\n\n\n\nThe p-values of the Kruskal-Wallis test and all pairwise tests are less than 0.05. There is thus sufficient evidence to reject H0 and conclude that the mean Reading score is significantly different for each group.\n\n\n\n\n\n\n\n\n\nInsights on Relationship between Schools and Reading Performance\n\n\n\n\nReading scores are lowest in Group 1 schools and highest in Group 4 schools. The difference in mean scores between the two groups is 143.6.\nReading scores increases in the following order: Group 1, Group 2, Group 3, Group 4.\n\n\n\n\n\n4.1.5 Relationship between Schools and Overall Performance\nFrom Sections 4.1.2 to 4.1.4, we observed that performance for all subjects increases from Group 1 through to 4. It may be the case that different schools specialise in different subjects, which does not seem to be a completely unfavourable scenario as students can choose their schools based on their passion in certain areas. This would mean that the schools in each group differs from subject to subject. However, if the composition of schools in each group remain relatively stable across subjects, it would mean that high performance is clustered within a select few schools and lower performance is more prevalent in other select few schools. This may be bad news as it shows that students may face huge stress to gain entry in such Group 4 schools while students from Group 4 schools may be labelled as having poor academic performance right from the get-go.\nIn particular, we will focus on Group 1 and Group 4 as they are at the extreme ends of the performance spectrum. A Venn Diagram will be used to visualise the number of schools that are common between each pair of school groups. This gives a clear visualisation of how many schools each pair of school groups have in common. The venn.diagram() function from the VennDiagram package will be used.\n\nGroup 1 SchoolsGroup 4 Schools\n\n\n\n\nCode\n# Preparing dataset\ng1_math &lt;- math_sch %&gt;% filter(group==\"Group 1\")\ng1_math$subject &lt;- \"Math\"\ng1_sci &lt;- sci_sch %&gt;% filter(group==\"Group 1\")\ng1_sci$subject &lt;- \"Science\"\ng1_read &lt;- read_sch %&gt;% filter(group==\"Group 1\")\ng1_read$subject &lt;- \"Reading\"\ng1 &lt;- rbind(g1_math[, c(1,4)], g1_sci[, c(1,4)], g1_read[, c(1,4)])\n\n# Plotting Venn Diagram\nvenn.diagram(\n  x=list(\n    g1 %&gt;% filter(subject==\"Math\") %&gt;% select(CNTSCHID) %&gt;% unlist(),\n    g1 %&gt;% filter(subject==\"Science\") %&gt;% select(CNTSCHID) %&gt;% unlist(),\n    g1 %&gt;% filter(subject==\"Reading\") %&gt;% select(CNTSCHID) %&gt;% unlist()\n  ), \n  category.names=c(\"Math\", \"Science\", \"Reading\"), \n  filename=\"venn_g1.png\", \n  output = TRUE ,\n          imagetype=\"png\" ,\n          height = 480 , \n          width = 480 , \n          resolution = 300,\n          compression = \"lzw\",\n          lwd = 1,\n          col=c(\"#440154ff\", '#21908dff', '#fde725ff'),\n          fill = c(alpha(\"#440154ff\",0.3), alpha('#21908dff',0.3), alpha('#fde725ff',0.3)),\n          cex = 0.5,\n          fontfamily = \"sans\",\n          cat.cex = 0.3,\n          cat.default.pos = \"outer\",\n          cat.pos = c(-27, 27, 135),\n          cat.dist = c(0.055, 0.055, 0.085),\n          cat.fontfamily = \"sans\",\n          cat.col = c(\"#440154ff\", '#21908dff', '#fde725ff'),\n          rotation = 1\n)\n\n\n\n\n\nVenn Diagram of Group 1 Schools\n\n\nFrom the Venn Diagram, we can see a significant overlap of schools in Group 1. Each subject has 41 schools in Group 1. Out of the 41 schools, 31 schools belong to Group 1 for all three subjects. This means that students with the lowest performance in all three subjects are largely concentrated in the same schools.\n\n\n\n\nCode\n# Preparing dataset\ng4_math &lt;- math_sch %&gt;% filter(group==\"Group 4\")\ng4_math$subject &lt;- \"Math\"\ng4_sci &lt;- sci_sch %&gt;% filter(group==\"Group 4\")\ng4_sci$subject &lt;- \"Science\"\ng4_read &lt;- read_sch %&gt;% filter(group==\"Group 4\")\ng4_read$subject &lt;- \"Reading\"\ng4 &lt;- rbind(g4_math[, c(1,4)], g4_sci[, c(1,4)], g4_read[, c(1,4)])\n\n# Plotting Venn Diagram\nvenn.diagram(\n  x=list(\n    g4 %&gt;% filter(subject==\"Math\") %&gt;% select(CNTSCHID) %&gt;% unlist(),\n    g4 %&gt;% filter(subject==\"Science\") %&gt;% select(CNTSCHID) %&gt;% unlist(),\n    g4 %&gt;% filter(subject==\"Reading\") %&gt;% select(CNTSCHID) %&gt;% unlist()\n  ), \n  category.names=c(\"Math\", \"Science\", \"Reading\"), \n  filename=\"venn_g4.png\", \n  output = TRUE ,\n          imagetype=\"png\" ,\n          height = 480 , \n          width = 480 , \n          resolution = 300,\n          compression = \"lzw\",\n          lwd = 1,\n          col=c(\"#440154ff\", '#21908dff', '#fde725ff'),\n          fill = c(alpha(\"#440154ff\",0.3), alpha('#21908dff',0.3), alpha('#fde725ff',0.3)),\n          cex = 0.5,\n          fontfamily = \"sans\",\n          cat.cex = 0.3,\n          cat.default.pos = \"outer\",\n          cat.pos = c(-27, 27, 135),\n          cat.dist = c(0.055, 0.055, 0.085),\n          cat.fontfamily = \"sans\",\n          cat.col = c(\"#440154ff\", '#21908dff', '#fde725ff'),\n          rotation = 1\n)\n\n\n\n\n\nVenn Diagram of Group 4 Schools\n\n\nFrom the Venn Diagram, we can see a even more significant overlap of schools in Group 4. Each subject has 41 schools in Group 4. Out of the 41 schools, 37 schools belong to Group 4 for all three subjects. This means that students with the highest performance in all three subjects are largely concentrated in the same schools.\n\n\n\n\n\n\n\n\n\nInsights on Relationship between Schools and Overall Performance\n\n\n\n\nStudents with the lowest performance across all subjects are largely concentrated in the same schools\nStudents with the highest performance across all subjects are also largely concentrated in the same schools"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#relationship-between-performance-and-gender",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#relationship-between-performance-and-gender",
    "title": "Is Every School a Good School?",
    "section": "4.2 Relationship between Performance and Gender",
    "text": "4.2 Relationship between Performance and Gender\nResearch shows that females and males excel in different areas. According to a study published in American Psychologist, researchers found that girls scored higher on literacy (or reading) tests than boys after reviewing data on 3.9 million literacy tests. Conversely, Wrigley-Asante et al. (2023) explained that the overall academic performance of males was rated higher than females.\nIn this section, we explore the relationship between gender and the performance of its students. Specifically, we ask if the gender affects the performance of students in terms of their performance in Mathematics, Science and Reading.\nIn the PISA dataset, the gender of students are recorded in the ST004D01T column. A quick check of the data using the n_distinct() function from the dplyr package revealed that there were only 2 unique values in the column - 1 for female and 2 for male. Using is.na() function from the {base} package also showed that there were no missing values. Therefore, the only data wrangling step performed was to recode the column to display ‘Female’ and ‘Male’ instead of values.\n\n\nCode\n# Recoding ST004D01T column \nstu_sg$ST004D01T[stu_sg$ST004D01T == '1'] &lt;- 'Female'\nstu_sg$ST004D01T[stu_sg$ST004D01T == '2'] &lt;- 'Male'\n\n\nThe distribution of subject scores for each gender group will be visualised using a ridgeline plot. As the plots are assigned to the same horizontal scale and presented with a slight overlap, the difference in scores between both genders will be made obvious by observing whether a gender’s distribution lies to the left or right of the distribution of the other gender.\n\n\n\n\n\n\nDesign Considerations for Ridgeline Plot\n\n\n\n\nThe quantile lines are plotted into ridgeline plot in order to make the median and interquartile range obvious. The geom_density_ridges_gradient() function was used to colour the plot via the calculated after_stat(quantile) aesthetic.\nThe transparency of the fill was increased so as to better see how the distributions between both genders overlaps by using the alpha argument.\n\n\n\n\nEDA PlotNormality Testt-Test\n\n\nFrom the ridgeline plots, it appears that females outperform males in Reading while males outperform females in Mathematics and Science.\n\n\nCode\n# Pivoting dataframe for facet_wrap()\ngender &lt;- stu_sg[, c(\"ST004D01T\", \"PV5MATH\", \"PV5SCIE\", \"PV5READ\")]\ngender &lt;- gender %&gt;% pivot_longer(cols=c(\"PV5MATH\", \"PV5SCIE\", \"PV5READ\"))\n# Renaming subjects\ngender$name[gender$name == \"PV5MATH\"] &lt;- \"Mathematics\" \ngender$name[gender$name == \"PV5SCIE\"] &lt;- \"Science\" \ngender$name[gender$name == \"PV5READ\"] &lt;- \"Reading\" \n\nggplot(data=gender, aes(x=value, y=as.factor(ST004D01T), fill=factor(after_stat(quantile)))) +\n  stat_density_ridges(geom=\"density_ridges_gradient\", \n                      calc_ecdf=TRUE,\n                      quantiles=4,\n                      quantile_lines=) +\n  scale_fill_viridis_d(name=\"Quartiles\", alpha=0.7) + \n  theme_ridges() + \n  \n  # Labelling axis and plot\n  ggtitle(\"Ridgeline Plot of Subject Scores By Gender\") + \n  xlab(\"PV5 Scores\") + \n  ylab(\"\") + \n  theme(axis.title.x=element_text(hjust=0.5, vjust=0.2)) + \n  \n  facet_wrap(~ name, ncol=1)\n\n\n\n\n\n\n\nTo ascertain our observations, tests for normality was first conducted on the distribution for each gender.\n\n\nCode\ngender %&gt;%\n  ggplot(aes(sample=value)) +\n  geom_qq() + geom_qq_line() + \n  facet_wrap(~name, scales=\"free_y\", nrow=1) +\n  labs(title=\"Normality Assumption Test\",\n       x=\"\", y=\"\") + \n  theme_bw() + \n  theme(axis.text.x=element_blank(), \n        axis.text.y=element_blank(), \n        panel.grid.major=element_blank(),\n        panel.grid.minor=element_blank(),\n        axis.ticks=element_blank())\n\n\n\n\n\nThe p-values computed are as shown in the table below.\n\n\nCode\nm &lt;- gender[gender$name==\"Mathematics\", ]\nad.test(m$value)\n\ns &lt;- gender[gender$name==\"Science\", ]\nad.test(s$value)\n\nr &lt;- gender[gender$name==\"Reading\", ]\nad.test(r$value)\n\n\n\n\n\n\nMathematics\nScience\nReading\n\n\n\n\np-value\n2.2e- 16\n2.2e- 16\n2.2e- 16\n\n\n\nSince all p-values are less than 0.05, the scores for all subjects do not follow a normal distribution across both genders. A non-parametric two-sample t-test will thus be used to compare the mean subject scores.\n\n\nThree Mann-Whitney U Tests, one for each subject, were used to compare the mean scores across genders, with the following hypothesis:\nH0: The mean subject score is the same for both genders.\nH1: The mean subject score is not the same for both genders.\nThe Mann-Whitney U Test was conducted using the ggbetweenstats() function from the ggstatsplot package.\n\n\nCode\nggplot() + \n  ggbetweenstats(data = gender[gender$name==\"Mathematics\", ],\n                 x = ST004D01T, y = value,\n                 type = \"np\",messages = FALSE, \n                 ylab=\"Mathematics Scores\", xlab=\"\") + \n   ggbetweenstats(data = gender[gender$name==\"Science\", ],\n                 x = ST004D01T, y = value,\n                 type = \"np\",messages = FALSE,\n                 ylab=\"Science Scores\", xlab=\"\") +\n  ggbetweenstats(data = gender[gender$name==\"Reading\", ],\n                 x = ST004D01T, y = value,\n                 type = \"np\",messages = FALSE,\n                 ylab=\"Reading Scores\", xlab=\"\") \n\n\n\n\n\nThe p-values for the three subjects are less than 0.05. There is thus sufficient evidence to reject H0 and conclude that the mean scores is significantly different for each gender across all subjects.\n\n\n\n\n\n\n\n\n\nInsights on Relationship between Gender and Performance\n\n\n\n\nFemales perform better than males in Reading, with a difference in median scores of 18.00.\nMales perform better than females in Mathematics and Science, with a difference in median scores of 16.45 and 11.96 respectively."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#relationship-between-socioeconomic-status-and-performance",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#relationship-between-socioeconomic-status-and-performance",
    "title": "Is Every School a Good School?",
    "section": "4.3 Relationship between Socioeconomic Status and Performance",
    "text": "4.3 Relationship between Socioeconomic Status and Performance\nIn 2019, Mendaki researchers found a strong link between socio-economic statuses and grades in Singapore. Through cluster analysis, they found that pupils with higher scores tended to be of a relatively higher socio-economic status while those with indicators related to having a lower socio-economic platform tended to perform worse. More recently, researchers from the Singapore University of Social Science (SUSS) found increasing evidence of the link between lower socio-economic status and lower academic achievement. This may be attributed to parents from higher socio-economic status having more knowledge, resourcefulness and involvement with their child’s education journey. Parents with financial means are also better able to provide resources such as tuition to enhance their children’s learning and better provide them with a head-start in life.\nIn this section, we explore the relationship between socio-economic status and the performance of its students. Specifically, we ask if the socio-economic status affects the performance of students in terms of their performance in Mathematics, Science and Reading.\nPISA proxies the socioeconomic status of students using the PISA Index of Economic, Social and Cultural Status (ESCS). The ESCS is a composite score based on three indicators - highest parental occupation (HISEI), parental education (PARENDINT) and home possessions (HOMEPOS).\n\n4.3.1 Binning ESCS Values\nESCS is a continuous variable that ranges from -3.5488 to 2.3674. To reduce the complexity of the data and facilitate subsequent analysis, ESCS will be binned into 5 categories - Low, Mid-low, Mid, Mid-High and High using the cut() function from the dplyr package.\n\nincome &lt;- stu_sg[, c(\"PV5MATH\", \"PV5SCIE\", \"PV5READ\", \"ESCS\")] %&gt;%\n  mutate(Income=cut(ESCS, breaks=5, \n                          labels=c(\"Low\", \"Mid-low\", \"Mid\", \"Mid-high\", \"High\"),\n                          include.lowest=TRUE))\n\n\n\n4.3.2 Finding Overall Performance of Students\nStudents in Singapore take various national examinations at the end of certain education milestones to proceed to the next education level. These exams include the Primary School Leaving Examination, GCE N(T) Level, GCE N(A) Level, GCE O-Level and GCE A-Level. To ensure a holistic education, national examinations require at least one subject to be taken in each of the domains of Language, Science and Mathematics. In order to do well in such examinations, students in Singapore have to do well in all three domains. We will thus study how socio-economic status of students affect their overall performance across all three subjects instead of looking into subject-based performance.\nThe overall performance of students in the three subjects can be found by finding the mean level of proficiency.\n\n\nCode\n# Binning Science scores into proficiency levels\nincome &lt;- income %&gt;%\n  mutate(Science = case_when(\n    PV5SCIE &lt;= 409.54 ~ \"1\", \n    PV5SCIE &lt;= 484.14 ~ \"2\",\n    PV5SCIE &lt;= 558.73 ~ \"3\",\n    PV5SCIE &lt;= 633.33 ~ \"4\",\n    PV5SCIE &lt;= 707.93 ~ \"5\",\n    PV5SCIE &gt; 707.93 ~ \"6\",\n    TRUE ~ NA_character_\n  ))\n\n# Binning Mathematics scores into proficiency levels\nincome &lt;- income %&gt;%\n  mutate(Math = case_when(\n    PV5MATH &lt;= 420.07 ~ \"1\", \n    PV5MATH &lt;= 482.38 ~ \"2\",\n    PV5MATH &lt;= 544.68 ~ \"3\",\n    PV5MATH &lt;= 606.99 ~ \"4\",\n    PV5MATH &lt;= 669.30 ~ \"5\",\n    PV5MATH &gt; 669.30 ~ \"6\",\n    TRUE ~ NA_character_\n  ))\n\n# Binning Reading scores into proficiency levels\nincome &lt;- income %&gt;%\n  mutate(Read = case_when(\n    PV5READ &lt;= 407.47 ~ \"1\", \n    PV5READ &lt;= 480.18 ~ \"2\",\n    PV5READ &lt;= 552.89 ~ \"3\",\n    PV5READ &lt;= 625.61 ~ \"4\",\n    PV5READ &lt;= 698.32 ~ \"5\",\n    PV5READ &gt; 698.32 ~ \"6\",\n    TRUE ~ NA_character_\n  ))\n\n# Finding mean overall performance\nincome$Math &lt;- as.numeric(income$Math)\nincome$Science &lt;- as.numeric(income$Science)\nincome$Read &lt;- as.numeric(income$Read)\n\nincome$overall &lt;- rowMeans(income[, 6:8])\n\n\n\n\n4.3.3 Visualising Relationship between Socioeconomic Status and Performance\nStripcharts are one-dimensional scatter charts. By showing the overall performance of individual students in each socioeconomic group, we can get a clear idea of how students in one group performs compared to students in another group. This way, the relationship between socioeconomic status and performance can be inferred.\n\n\n\n\n\n\nDesign Considerations for Stripchart\n\n\n\n\nThe degree of jitter was lowered using the position argument in geom_jitter() to reduce the horizontal spacing between points in each income group for better visualisation of how the points are spread vertically. The vertical spread of the data is more important here as it shows how the overall performance varies within and across groups.\nThe transparency of points was lowered using the alpha argument in geom_jitter(). Since the points would tend to cluster, using a lower alpha value would enable visibility of points. It also creates a visual density, where higher proportion of overlaps produce darker colours.\nThe mean and standard deviation for each group was added using mean_sdl in the stat_summary() function. This allows the interquartile range to be easily seen.\n\n\n\n\nEDA PlotNormality TestANOVA\n\n\nBased on the stripchart, it appears that overall performance increases across income groups. Income group Low recorded the lowest performance while income group High recorded the highest performance\n\n\nCode\nggplot(income, aes(x=Income, y=overall, color=Income)) + \n  # Adjust degree of jitter in x-direction\n  geom_jitter(position=position_jitter(0.2), cex=1, alpha=0.25) +\n  # Adding mean and standard deviation\n  stat_summary(fun.data=\"mean_sdl\", mult=1, \n               geom=\"pointrange\", color=\"red\")\n\n\n\n\n\n\n\n\nincome %&gt;%\n  group_by(Income) %&gt;%\n  shapiro_test(overall) %&gt;% \n  kbl() %&gt;%\n  kable_styling(full_width=FALSE, bootstrap_options = \"responsive\")\n\n\n\n\n\nIncome\nvariable\nstatistic\np\n\n\n\n\nLow\noverall\n0.8776739\n0.0002052\n\n\nMid-low\noverall\n0.9502238\n0.0000000\n\n\nMid\noverall\n0.9763097\n0.0000000\n\n\nMid-high\noverall\n0.9628418\n0.0000000\n\n\nHigh\noverall\n0.9256653\n0.0611401\n\n\nNA\noverall\n0.9339609\n0.0105892\n\n\n\n\n\n\n\n\nSince the p-values of all income groups are less than 0.05, overall performance levels do not follow a normal distribution across all income groups. A non-parametric ANOVA test will thus be used to compare the mean overall performance levels.\n\n\nA non-parametric Kruskal-Wallis One-Way ANOVA test was used to compare the mean overall performance across income groups, with the following hypotheses:\nH0: The mean overall performance is the same for all income groups.\nH1: The mean overall performance is not the same for all income groups.\n\nggbetweenstats(\n  data=income,x=Income, y=overall,\n  type=\"nonparametric\",plot.type=\"box\",\n  pairwise.comparisons=TRUE, pairwise.display=\"significant\", \n  centrality.plotting=FALSE, bf.message=FALSE)\n\n\n\n\nThe p-values of the Kruskal-Wallis test and all pairwise tests are less than 0.05. There is thus sufficient evidence to reject H0 and conclude that the mean overall performance is significantly different for each income group.\n\n\n\n\n\n\n\n\n\nInsights of Relationship between Income and Overall Performance\n\n\n\n\nOverall performance across the three subjects are different for each income group, with the Low Income group performing the worst and the High Income group performing the best.\nOverall performance increases with income as evident from performance increasing the in the following order: Low Income, Mid-low Income, Mid Income, Mid-high Income and High Income."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#conclusion",
    "href": "Take_Home_Exercise/Take_Home_Ex01/Take_Home_Ex01.html#conclusion",
    "title": "Is Every School a Good School?",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nIn this exercise, we used statistical plots to understand and analyse the performance of Singapore students in the PISA survey.\nIn Section 4, we explored the relationship between performance and schools, gender and socio-economic status. Statistical tests were also conducted to confirm the findings. The following findings were made:\n\nSchools affect how students in perform in all subjects, with Group 1 schools recording the best performance and Group 4 schools recording the worst performance.\nStudents with the lowest performance across all subjects are largely concentrated in the same schools. The same can be said about students with the highest performance across all subjects.\nFemales perform better than their male counterparts in Reading while males perform better than their female counterparts in Mathematics and Science.\nOverall performance increases with income levels, where students from lower-income families perform worse than their peers from higher-income families.\n\nWhile the analysis presented in this exercise is not exhaustive, it should provide some insights to Singapore’s education system and identified certain areas to work on so as to progress towards a more equitable system that will allow students from all backgrounds to excel."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html",
    "href": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html",
    "title": "DataVis Makeover",
    "section": "",
    "text": "The original dataset was downloaded from the PISA 2022 Database, titled Student Questionnaire Data File. The file downloaded was cy08msp_stu_qqq.sas7bdat."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#preliminary-steps",
    "href": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#preliminary-steps",
    "title": "DataVis Makeover",
    "section": "1. Preliminary Steps",
    "text": "1. Preliminary Steps\n\n1.1 Importing Relevant R Packages\n\npacman::p_load(haven, tidyverse, ggplot2, dplyr, readr, ggpubr,\nscales,forcats, gridExtra, corrplot, Hmisc, lubridate,knitr, kableExtra,patchwork, \nggiraph, ggrepel)\n\n\n\n1.2 Preparing the Data\nAs instructed, the original data prepared by the peer should be left unchanged as much as possible unless there is value-add in supplementing the data. The data preparation steps taken by the student will thus be replicated, with adjustments made in later steps as needed.\n\n\nCode for importing data\nstu_qqq &lt;- read_sas(\"Data/cy08msp_stu_qqq.sas7bdat\")\nstu_sg &lt;- stu_qqq %&gt;% filter(CNT == \"SGP\")\nwrite_rds(stu_sg, \"Data/stu_SG.rds\")\n\n\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_sg.rds\")\n\n34 variables relating to the Student ID, School ID, gender of students, ESCS index of students, and subject performance across Mathematics, Science and Reading were extracted into a new table stu_qqq_SG_01.\n\n\nCode for extracting variables\nstu_qqq_SG_01 &lt;- stu_qqq_SG[, c(\"CNTSTUID\", \"CNTSCHID\", \"ST004D01T\", \"ESCS\",\n                             \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\", \n                             \"PV6MATH\", \"PV7MATH\", \"PV8MATH\", \"PV9MATH\", \"PV10MATH\", \n                             \"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\", \n                             \"PV6READ\", \"PV7READ\", \"PV8READ\", \"PV9READ\", \"PV10READ\", \n                             \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\", \n                             \"PV6SCIE\", \"PV7SCIE\", \"PV8SCIE\", \"PV9SCIE\", \"PV10SCIE\")]\n\nwrite_rds(stu_qqq_SG_01,\n          \"data/stu_qqq_SG_01.rds\")\n\n\nThe average Plausible Value (PV) was found for each subject. The overall performance for each student across the three subjects was also found by finding the average of the mean of the three subjects.\n\n\nCode for averaging PVs\nstu_qqq_SG_01 &lt;- stu_qqq_SG_01 %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    MeanMATH = mean(c(PV1MATH, PV2MATH, PV3MATH, PV4MATH, PV5MATH, \n                      PV6MATH, PV7MATH, PV8MATH, PV9MATH, PV10MATH), na.rm = TRUE),\n    MeanREAD = mean(c(PV1READ, PV2READ, PV3READ, PV4READ, PV5READ, \n                      PV6READ, PV7READ, PV8READ, PV9READ, PV10READ), na.rm = TRUE),\n    MeanSCIE = mean(c(PV1SCIE, PV2SCIE, PV3SCIE, PV4SCIE, PV5SCIE, \n                      PV6SCIE, PV7SCIE, PV8SCIE, PV9SCIE, PV10SCIE), na.rm = TRUE),\n    MeanOverall=mean(c(PV1MATH, PV2MATH, PV3MATH, PV4MATH, PV5MATH, \n                      PV6MATH, PV7MATH, PV8MATH, PV9MATH, PV10MATH,\n                      PV1READ, PV2READ, PV3READ, PV4READ, PV5READ, \n                      PV6READ, PV7READ, PV8READ, PV9READ, PV10READ,\n                      PV1SCIE, PV2SCIE, PV3SCIE, PV4SCIE, PV5SCIE, \n                      PV6SCIE, PV7SCIE, PV8SCIE, PV9SCIE, PV10SCIE), na.rm = TRUE)\n  ) %&gt;%\nungroup() %&gt;%\nselect(-matches(\"^PV\\\\d+MATH$\"), -matches(\"^PV\\\\d+READ$\"), \n       -matches(\"^PV\\\\d+SCIE$\"), MeanMATH, MeanREAD, MeanSCIE,MeanOverall)"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#data-visualisation-critique",
    "href": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#data-visualisation-critique",
    "title": "DataVis Makeover",
    "section": "2. Data Visualisation Critique",
    "text": "2. Data Visualisation Critique\nThe critique and improvements proposed will be based on clarity and aesthetics."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#scatter-plot-of-score-against-meanoverall",
    "href": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#scatter-plot-of-score-against-meanoverall",
    "title": "DataVis Makeover",
    "section": "2.1 Scatter Plot of Score Against MeanOverall",
    "text": "2.1 Scatter Plot of Score Against MeanOverall\nIn the original scatterplot designed, the mean overall score and mean Science scores were plotted against the ESCS index to visualise the relationship between the overall scores and Science scores with socioeconomic status as shown below.\n\n\nCode for original scatterplot\nggplot(stu_qqq_SG_01, aes(x = ESCS)) +\n  geom_point(aes(y = MeanOverall, color = \"Overall\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanOverall, color = \"Overall\"), method = 'lm') +\n  geom_point(aes(y = MeanSCIE, color = \"Science\"), alpha = 0.5) +\n  geom_smooth(aes(y = MeanSCIE, color = \"Science\"), method = 'lm') +\n  scale_color_manual(values = c(\"Overall\" = \"lightseagreen\", \"Science\" = \"palevioletred1\"),\n                     name = \"Subject\", \n                     labels = c(\"Overall\" = \"Mean Overall\", \"Science\" = \"Mean Science\")) +\n  theme_minimal() +\n  labs(title = \"Relationship Between ESCS and Academic Performance\",\n       x = \"ESCS Index\", y = \"Score\")\n\n\n\n\n\n\n2.1.1 Critique of Scatterplot\n\nClarityAesthetics\n\n\n\nThe choice of a scatterplot with trend lines was good for visualising the relationship between the overall performance and subject-specific performance.\nDue to the numerous records, the points are highly clustered together in the plot. Coupled with the relatively high opacity of the points (alpha=0.5), the middle region of the plot is obscured. The points for overall performance and Science performance cannot be differentiated due to the high density of markers in the middle region. Further, the concentrated markers are covering the two trend lines. Important information may thus not be visible.\nThe confidence intervals of the trend lines overlap largely due to the close proximity of the two lines. This makes it impossible to differentiate the confidence intervals of the lines.\nThe title is descriptive but serves little purpose in explaining the visualisation. It does not tell the audience the context, such as the main highlight or story.\nThe axes label does not aid the audience in interpreting the graph as Mean Overall does not explain anything if there is no context provided on the data preparation steps.\nThere is no caption to present the source of the data.\n\n\n\n\nThe white background ensures the contrast between the background and the plot elements is obvious, making the plot stand out clearly.\nThe soft grid lines used ensure minimal distraction from the data.\nThere are no tick marks, making it rather hard to read the axes especially since both axes are using a continuous scale.\nThere is significant white space due to the legend.\n\n\n\n\n\n\n2.1.2 Proposed Improvements for Scatterplot\nOne of the main problems of the original visualisation is the highly cluttered points obscuring important information. In depicting the relationship between academic performance and socioeconomic status, the trend lines are the most salient and should draw the most attention. In contrast, the individual scores should remain in the background and only serve to supplement the trend line.\nTo facilitate this, the trend line for overall performance and Science performance can be plotted using a darker shade compared to its corresponding subject marker. The score markers can be made more transparent by increasing alpha and its border can be made thinner using stroke so that the markers do not overwhelm the plot area. A sketch of the proposed changes is shown below. Furthermore, the confidence interval of each line can be differentiated by setting its colour to correspond to its marker and trend line using fill.\n\n\n\nProposed Sketch of Improved Scatterplot\n\n\n\n\n2.1.3 Makeover of Scatterplot\n\n\n\n\n\n\nggplot2 Explanation\n\n\n\nFor the score markers for overall and Science performance:\n\nThe transparency of the score markers was increased by setting alpha=0.15.\nThe size of the score markers was decreased by setting size=1.\nThe border around the score markers was removed by setting stroke=NA.\n\nFor the trend lines for overall performance and Science:\n\nThe colour of the trend line was set to a darker shade by setting the color argument to specific RGB codes.\nThe colour of the confidence interval was set to a shade lighter than the trend line by setting the fill argument to specific RGB codes.\n\nFor other aspects of the plot,\n\nThe title and subtitle were added using the ggtitle() function. The appearance and position of the titles were adjusted using the plot.title argument of the theme() function. Appearance can be controlled with options such as family, size or color while position can be controlled with hjust and vjust. The title and subtitle position was set respect to the whole plot instead of the panel using the plot.title.position component of the theme function.\nThe axes labels were added using scale_y_continuous and scale_x_continuous.\nThe legend was adjusted using the legend.position, legend.title and legend.text arguments of the theme() function. As the marker size is set to 1 in the geom_point(), the markers are displayed as-is in the legend. This makes the legend rather hard to read as the points are very small. To change the size of the markers shown in the legend, guide() and guide_legend() was used to override the aesthetic parameters stated in geom_point() so the aesthetic parameters of the legend key can be specified instead.\nThe caption was added using the caption argument in the labs() function.\n\n\n\n\nMakeover PlotCode\n\n\n\n\n\n\nggplot(data=stu_qqq_SG_01, aes(x=ESCS)) + \n  scale_color_manual(values = c(\"Overall Score\"=\"lightseagreen\", \"Science Score\"=\"palevioletred1\")) +\n  theme_bw() +\n  \n  geom_point(aes(y = MeanOverall, color = \"Overall Score\"), \n             # Increasing transparency of marker\n             alpha=0.15, \n             # Decreasing size of marker\n             size=1,\n             # Removing border around marker\n             stroke=NA) +\n  geom_point(aes(y=MeanSCIE, color=\"Science Score\"), alpha=0.15, size=1, stroke=NA) +\n  \n  geom_smooth(aes(y=MeanOverall), method=\"lm\", \n              # Setting colour of trendline to darker shade \n              color='#F26B8A', \n              # Setting colour of confidence interval to correspond with markers and line\n              fill='#FE7F9C') + \n  geom_smooth(aes(y=MeanSCIE), method=\"lm\", color='#028A0F', fill='#5DBB63') + \n   \n  # Adding a title and subtitle\n  ggtitle(label=\"Socio-economically advantaged students perform better at both the subject \\nlevel and overall level\",\n       subtitle=\"Mean scores (Overall and Science) against ESCS index\") +\n  theme(plot.title=element_text(face='bold', size=13), \n        plot.subtitle=element_text(size=10),\n        plot.title.position=\"plot\") + \n  \n  # Adjusting axes\n  scale_x_continuous(name=\"ESCS Index\") + \n  scale_y_continuous(name=\"Mean Score\") +\n  \n  # Adjusting legend\n  theme(legend.position=\"top\", \n        legend.title=element_blank(), \n        legend.text=element_text(size=8)) +\n  guides(color=guide_legend(override.aes=list(size=4))) + \n  \n  # Adding caption\n  labs(caption=\"Source: PISA 2022 Database - Student questionnaire data file\", size=6)\n\n\n\n\n\n\n2.1.4 Interactivity of Scatterplot\nHowever, there are still overlaps of elements in the proposed scatterplot. The two trend lines are extremely close together, causing one to cover the other significantly. There are two possible solutions to this issue:\n\nRemove one of the trend lines. As both trend lines are almost running in parallel and extremely close to each other, some might argue they are displaying the same relationship between academic performance and socioeconomic status. Having both the subject-specific and overall trend lines may thus be excessive. However, only retaining the trend line for overall performance hides valuable subject-level information. It may be the case where one subject performance displays a relationship with socioeconomic status opposite to that of the other two but its effects was cancelled out when overall performance was found. Only retaining the trend line for Science would not inform readers of the relationship with overall performance, which is likely the main concern for most.\nIntroduce interactivity in the plots. The trend lines for overall score and Science scores can be differentiated using hover effects where the reader can hover their mouse over the visualisation to highlight one trend line.\n\nThe below tabset show details the preliminary steps taken to develop the interactive scatterplot.\n\nCode for Data PreparationCode for Plotting\n\n\nTo prepare for interactive data visualisation, the data was converted from wide to long format by pivoting the MeanSCIE and MeanOverall columns to a single column and adding a new column for Subject.\n\ncolnames(stu_qqq_SG_01)[7] = \"Overall\"\ncolnames(stu_qqq_SG_01)[8] = \"Science\"\n\nstu_qqq_SG_02 &lt;- pivot_longer(stu_qqq_SG_01[, c(4, 7, 8)], cols=c(\"Overall\", \"Science\"))\n\ncolnames(stu_qqq_SG_02)[2] = \"Subject\"\ncolnames(stu_qqq_SG_02)[3] = \"Score\"\n\nstu_qqq_SG_02 &lt;- na.omit(stu_qqq_SG_02)\n\n\n\n\nplot2 &lt;- \nggplot(data=stu_qqq_SG_02, aes(x=ESCS, y=Score)) +\n  theme_bw() +\n  ggtitle(label=\"Socio-economically advantaged students perform better at both the \\nsubject level and overall level\",\n       subtitle=\"Mean scores (Overall and Science) against ESCS index\") +\n  theme(plot.title=element_text(face='bold', size=13), \n        plot.subtitle=element_text(size=10),\n        plot.title.position=\"plot\") + \n  scale_x_continuous(name=\"ESCS Index\") + \n  scale_y_continuous(name=\"Mean Score\") +\n  theme(legend.position=\"top\", \n        legend.title=element_blank(), \n        legend.text=element_text(size=8)) +\n  guides(color=guide_legend(override.aes=list(size=4))) + \n  labs(caption=\"Source: PISA 2022 Database - Student questionnaire data file\", size=6) + \n  \n  geom_point_interactive(aes(data_id=Subject, tooltip=Subject, color=Subject),\n                         alpha=0.15, size=1, stroke=NA) + \n  \n  geom_smooth_interactive(aes(data_id=Subject, tooltip=Subject, color=Subject),\n                         method=\"lm\", se=FALSE,\n                         linewidth=0.5)\n\n\n\n\n\n\n2.1.5 Final Visualisation for Scatterplot\n\n\n\n\n\n\nIntroducing Interactivity\n\n\n\nIn the revised improved scatterplot, the interactivity of the plot was introduced by using geom_point_interactive() and geom_smooth_interactive(). The user can toggle between the trend lines for overall performance and Science performance. To illustrate, when the user places his/her cursor over the trend line or any marker for overall performance, the trend line for overall performance and the score markers for overall performance will be highlighted in pink and purple respectively. Elements irrelevant to overall performance (trend line and score markers for Science performance) will be greyed out. This allows for increased clarity for overall performance by reducing the clutter visible to the reader.\n\n\n\ngirafe(ggobj=plot2, \n       width_svg=6, \n       height_svg=6*0.618, \n       options=list(\n         opts_hover(css=\"fill:purple;\"), \n         opts_hover_inv(css=\"fill:grey; opacity:0.2;\")))"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#strip-chart-of-school-and-academic-performance",
    "href": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#strip-chart-of-school-and-academic-performance",
    "title": "DataVis Makeover",
    "section": "2.2 Strip Chart of School and Academic Performance",
    "text": "2.2 Strip Chart of School and Academic Performance\nIn the original stripchart designed, the mean score of each subject was plotted against the student’s School ID to visualise the relationship between the student’s school and academic performance as shown below.\n\n\nCode\ncolnames(stu_qqq_SG_01)[7] = \"MeanOverall\"\ncolnames(stu_qqq_SG_01)[8] = \"MeanSCIE\"\n\nlong_data &lt;- stu_qqq_SG_01 %&gt;%\n  gather(key = \"Subject\", value = \"Score\", MeanMATH, MeanREAD, MeanSCIE, MeanOverall) %&gt;%\n  group_by(CNTSCHID, Subject) %&gt;%\n  summarise(AverageScore = mean(Score, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Label = ifelse(AverageScore &gt; 650 | AverageScore &lt; 450, as.character(CNTSCHID), \"\"))\n\n# Create a dot plot to compare the average performance of different schools\nggplot(long_data, aes(x = CNTSCHID, y = AverageScore, color = Subject)) +\n  geom_point(position = position_dodge(width = 0.5)) +\n  geom_text(aes(label = Label), check_overlap = TRUE, vjust = -1, position = position_dodge(width = 0.5)) +\n  theme_minimal() +\n  labs(title = \"Comparison of School Performance in Subjects\",\n       x = \"School ID\", y = \"Average Score\") +\n  scale_color_brewer(palette = \"Set1\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) \n\n\n\n\n\n\n2.2.1 Critique of Stripchart\n\nClarityAesthetics\n\n\n\nWith 167 schools each having 4 points, the large number of points clutters the entire plot and causes any valuable insights to be unintuitive. The relationship between the choice of school and academic performance is rather hidden and careful scrutiny of the plot is required to decipher the relationship.\nThe x-axis plots the 8-digit School ID in ascending order, which serves little purpose for two reasons. First, the first five digits of 70200 are the same for all schools in Singapore as it is the 6-digits National Centre Code. The unique identifier for schools is only the last three digits, indicating that retaining the whole 8-digit code is only adding on to the clutter in the plot and has little value-add. Second, no contextual background was provided by PISA on the School IDs. Therefore, simply sorting the x-axis of School IDs in ascending order does not aid in increasing the clarity or interpretability of the visualisation.\nFour points were plotted for each school, namely the mean scores for Science, Reading, Mathematics and overall performance. However, the mean overall score was computed as the average of the other 3 subject scores. With the overall score being a linearly derived value of the other 3 variables, it would not only clutter the data but also not provide any additional valuable insights.\nThe title is descriptive but serves little purpose in explaining the visualisation. It does not tell the audience the context, such as the main highlight or story.\nThe axes label does not aid the audience in interpreting the graph as MeanMATH, MeanOverall, MeanREAD and MeanSCIE does not explain anything if there is no context provided on the data preparation steps.\nThere is no caption to present the source of the data.\n\n\n\n\nThe white background ensures the contrast between the background and plot elements is obvious, making the plot stand out clearly.\nThe soft grid lines used ensure minimal distraction from the data.\nThere is significant white space due to the legend.\nThe x-axis labels are vertical, which makes it rather hard to read.\n\n\n\n\n\n\n2.2.2 Proposed Improvements for Strip Chart\nThe first main improvement would be to remove the first five digits of the School ID for all records as only the last three digits serve as the unique identifier for schools in Singapore. This will help to significantly reduce the clutter around the x-axis and plot area when using the School ID to label the axis and salient data points.\nThe second main improvement would be to include 15th and 85th quartiles of the overall scores in the visualisation. Data points lying within the stipulated range coloured grey as they are less relevant in finding the relationship between schools and academic performance as they form the majority of the data and provides no value in identifying schools that has exceptionally high or low performances. By colouring such points grey and leaving data points falling outside the specified coloured, it draws attention to the more salient outlier points and reduces the clutter introduced by the less relevant majority points. Furthermore, using quartiles to identify schools with relatively higher or lower performance is a more data-driven approach compared to the peer’s method of using absolute values derived from observation of the visualisation.\nThe third main improvement would be to sort the x-axis (hence School IDs) by the mean overall performance. This way, the schools at the extreme ends with relatively lower or higher academic performance can be more easily identified.\n\n\n\nProposed Improved Strip Chart\n\n\n\n\n2.2.3 Makeover of Scatterplot\n\n\n\n\n\n\nggplot2 Explanation\n\n\n\n\nThe interquartile range was added using the annotate() function from the ggplot2 package. annotate() was used instead of geom_rect() as geom_rect() expects the same amount of data as that passed to ggplot(). It will thus plot hundreds of rectangles above each other, causing lowering the alpha to have little to no impact on the transparency of the rectangle. In contrast, annotate() draws the exact amount of rectangles as specified by the coordinates. This often having only one or few rectangles drawn such that its transparency can be much easily adjusted.\nThe data points that falls within the interquartile range was coloured grey by creating a category “In Range” in the Subject column. If the score falls within the 15th and 85th quantile, its subject records was replaced with”In Range” in the Subject column. The colours were then manually set using scale_color_manual() to assign grey to the “In Range” category in Subject.\nThe x-axis was sorted based on ascending order of mean overall scores using the fct_reorder() function.\nThe x-axis labels and ticks were removed as labelling all 167 schools will only clutter up the plot area.\nThe labels for schools with two or more subjects falling outside the 15th and 85th quartiles were added using geom_text_repel() instead of geom_text() as it helps repel text labels away from each other so that the labels do not interfere with each other.\nVertical grid lines were removed using panel.grid.major() as the numerous lines was cluttering the visualisation.\n\n\n\n\nCode for Data PreparationCode for Plotting\n\n\n\n# Removing first 5 digits of CNTSCHID\nlong_data$CNTSCHID &lt;- substr(long_data$CNTSCHID, 6, 8)\n\n# Removing MeanOverall \nlong_data &lt;- long_data[-which(long_data$Subject %in% \"MeanOverall\"), ]\n\n# Renaming subject records\nlong_data$Subject &lt;- ifelse(long_data$Subject == \"MeanREAD\", \"Reading\", long_data$Subject)\nlong_data$Subject &lt;- ifelse(long_data$Subject == \"MeanMATH\", \"Mathematics\", long_data$Subject)\nlong_data$Subject &lt;- ifelse(long_data$Subject == \"MeanSCIE\", \"Science\", long_data$Subject)\n\n# Finding 15th and 85th percentile for MeanOverall \nqr1 &lt;- quantile(stu_qqq_SG_01$MeanOverall, c(0.15, 0.85))[[\"15%\"]]\nqr2 &lt;- quantile(stu_qqq_SG_01$MeanOverall, c(0.15, 0.85))[[\"85%\"]]\n\n# Checking if point is within range \nlong_data$Subject &lt;- ifelse(long_data$AverageScore&lt;=qr2 & \n                              long_data$AverageScore&gt;=qr1, \n                            \"In Range\", long_data$Subject)\n\n# Adding column for MeanOverall\noverall &lt;- stu_qqq_SG_01 %&gt;% select(CNTSCHID, MeanOverall)\noverall$CNTSCHID &lt;- substr(overall$CNTSCHID, 6, 8)\ndf &lt;- overall %&gt;% group_by(CNTSCHID) %&gt;% summarise(overall = mean(MeanOverall))\nlong_data &lt;- merge(long_data, df, by=\"CNTSCHID\")\n\n# Finding schools with 2 or more points outside of range\nlong_data$in_range &lt;- ifelse(long_data$AverageScore&lt;=qr2 & long_data$AverageScore&gt;=qr1, 1, 0)\ncheck &lt;- long_data %&gt;% group_by(CNTSCHID) %&gt;% summarise(sum=sum(in_range)) \ncheck &lt;- check[rep(seq_len(nrow(check)), each=3), ] \nlong_data$in_range &lt;- ifelse(check$sum&gt;=2, 0, 1)\nlong_data$Label &lt;- ifelse(long_data$in_range==1 & long_data$Subject==\"Mathematics\", long_data$CNTSCHID, \"\")\n\n\n\n\nggplot(long_data, aes(x=fct_reorder(CNTSCHID, overall), y=AverageScore, color=Subject)) +\n  theme_bw() + \n  scale_x_discrete(expand=c(-1, 170)) +\n    scale_y_continuous(limit=c(329, 750)) +\n  # Removing vertical grid lines \n  theme(panel.grid.major = element_blank(),\n        axis.text.x=element_blank(), \n        axis.ticks.x=element_blank()) +\n  \n  geom_point(position = position_dodge(width=0.5)) +\n  \n  # Shading interquartile range\n  annotate(\"rect\", alpha=0.4, fill=\"grey\",\n           xmin=-Inf, xmax=Inf,\n           ymin=qr1,\n           ymax=qr2) +\n  \n  # Colouring points within IQR grey\n  scale_color_manual(values=c(\"Mathematics\"=\"palevioletred1\", \n                              \"Science\"=\"lightseagreen\", \n                              \"Reading\"=\"lightblue\", \n                              \"In Range\"=\"lightgrey\")) + \n  \n  # Labelling plot and axes\n  labs(title=\"Choice of school affects academic performance\", \n       subtitle=expression(atop(\n         \"Mean Subject Scores against School ID\", \n         scriptstyle(italic(\"Schools with two or more subjects falling outside the 15th and 85th percentile are labelled in the plot.\")))), \n       caption=\"Source: PISA 2022 Database - Student questionnaire data file\", size=6, \n       x=\"School\\n(sorted in ascending order of Mean Overall Score)\", \n       y=\"Mean Subject Score\") + \n  theme(plot.title=element_text(face='bold', size=13, hjust=0.5), \n        plot.subtitle=element_text(size=10, hjust=0.5),\n        plot.title.position=\"plot\") + \n  \n  # Changing position of legend\n  theme(legend.position=\"top\", \n      legend.title=element_blank(), \n      legend.text=element_text(size=8)) +\n    scale_fill_manual(\n      values=c(\"palevioletred1\", \"lightblue\", \"lightseagreen\", \"lightgrey\"), \n      breaks=c(\"Mathematics\", \"Reading\", \"Science\")) +\n  \n  # Labelling schools with 2 or more subjects \n  geom_text_repel(aes(label = Label), \n            vjust = -1, \n            position = position_dodge(width = 0.5), \n            color=\"black\", \n            size=3) \n\n\n\n\n\n\n2.2.4 Final Visualisation for Strip Chart\n\n\nCode\nggplot(long_data, aes(x=fct_reorder(CNTSCHID, overall), y=AverageScore, color=Subject)) +\n  theme_bw() + \n  scale_x_discrete(expand=c(-1, 170)) +\n    scale_y_continuous(limit=c(329, 750)) +\n  # Removing vertical grid lines \n  theme(panel.grid.major = element_blank(),\n        axis.text.x=element_blank(), \n        axis.ticks.x=element_blank()) +\n  \n  geom_point(position = position_dodge(width=0.5)) +\n  \n  # Shading interquartile range\n  annotate(\"rect\", alpha=0.4, fill=\"grey\",\n           xmin=-Inf, xmax=Inf,\n           ymin=qr1,\n           ymax=qr2) +\n  \n  # Colouring points within IQR grey\n  scale_color_manual(values=c(\"Mathematics\"=\"palevioletred1\", \n                              \"Science\"=\"lightseagreen\", \n                              \"Reading\"=\"lightblue\", \n                              \"In Range\"=\"lightgrey\")) + \n  \n  # Labelling plot and axes\n  labs(title=\"Choice of school affects academic performance\", \n       subtitle=expression(atop(\n         \"Mean Subject Scores against School ID\", \n         scriptstyle(italic(\"Schools with two or more subjects falling outside the 15th and 85th percentile are labelled in the plot.\")))), \n       caption=\"Source: PISA 2022 Database - Student questionnaire data file\", size=6, \n       x=\"School\\n(sorted in ascending order of Mean Overall Score)\", \n       y=\"Mean Subject Score\") + \n  theme(plot.title=element_text(face='bold', size=13, hjust=0.5), \n        plot.subtitle=element_text(size=10, hjust=0.5),\n        plot.title.position=\"plot\") + \n  \n  # Changing position of legend\n  theme(legend.position=\"top\", \n      legend.title=element_blank(), \n      legend.text=element_text(size=8)) +\n    scale_fill_manual(\n      values=c(\"palevioletred1\", \"lightblue\", \"lightseagreen\", \"lightgrey\"), \n      breaks=c(\"Mathematics\", \"Reading\", \"Science\")) +\n  \n  # Labelling schools with 2 or more subjects \n  geom_text_repel(aes(label = Label), \n            vjust = -1, \n            position = position_dodge(width = 0.5), \n            color=\"black\", \n            size=3)"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#learning-points",
    "href": "Take_Home_Exercise/Take_Home_Ex02/Take_Home_Ex02.html#learning-points",
    "title": "DataVis Makeover",
    "section": "3. Learning Points",
    "text": "3. Learning Points\nIn this exercise, ggplot2 and its extensions were used to critique and makeover peer works, which aids in the implementation of best practices in data visualisation. It is important to take a step back occasionally while building visualisations to ensure that the visualisation continues to be clear and intuitive. For example, the second visualisation was overly cluttered with the numerous points and excessively long data labels. It would thus be helpful to step back to think whether all elements included in the plot are essential in delivering our message. In our case, the points falling within the 15th and 85th quantiles were redundant in showing that the choice of schools affect academic performance while the length of the data labels could be reduced by removing the first five digits as they did not provide any information on the school."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "Welcome to ISSS608 Visual Analytics and Applications. In this website, you will find my coursework prepared for this course."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html",
    "href": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "This chapter introduces several ggplot2 extensions for creating more elegant and effective statistical graphics. Specifically, the following will be explained:\n\nControlling the placement of annotation on a graph by using functions provided in ggrepel package,\nCreating professional publication quality figures by using functions provided in ggthemes and hrbrthemes packages, and\nPlotting composite figures by combining ggplot2 graphs using patchwork package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#overview",
    "title": "ISSS608-VAA",
    "section": "",
    "text": "This chapter introduces several ggplot2 extensions for creating more elegant and effective statistical graphics. Specifically, the following will be explained:\n\nControlling the placement of annotation on a graph by using functions provided in ggrepel package,\nCreating professional publication quality figures by using functions provided in ggthemes and hrbrthemes packages, and\nPlotting composite figures by combining ggplot2 graphs using patchwork package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#getting-started",
    "title": "ISSS608-VAA",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\n\n2.2.1 Installing and loading required libraries\nFour R packages will be used: - ggrepel: provides geoms for ggplot2 to repel overlapping text labels - ggthemes: provides some extra themes, geoms and scales for ggplot2 - hrbrthemes: provides typography-centric themes and theme components for ggplot2 - patchwork: prepares composite figures creating using ggplot2\nThe code chunk below will be used to check if these packages have been installed and will load them onto the working R environment.\n\npacman::p_load(ggrepel, patchwork, ggthemes, hrbrthemes, tidyverse)\n\n\n\n2.2.2 Importing data\nFor the purpose of this exercise, a data file called Exam_data will be used. It consists of year end examination grades of a cohort of primary 3 students from a local school. It is in csv file format.\nThe code chunk below imports exam_data.csv into R environment by using read_csv() function of readr package. readr is one of the tidyverse package.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nThere are a total of seven attributes in the exam_data tibble data frame. Four of them are categorical data tyle and the other three are in continuous data tyle. - The categorical attributes are: ID, CLASS, GENDER and RACE. - The continuous attributes are: MATHS, ENGLISH and SCIENCE.\n\n\n2.3 Beying ggplot2 Annotation: ggrepel\nOne of the challenge of plotting statistical graph is annotation, especially with large number of data points.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\nggrepel is an extension of ggplot2 package to provide geoms for ggplot2 to repel overlapping text as in out example above. We simply replace geom_text() with geom_text_repel() and geom_label() with geom_label_repel().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#beyond-ggplot2-themes",
    "href": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#beyond-ggplot2-themes",
    "title": "ISSS608-VAA",
    "section": "2.4 Beyond ggplot2 Themes",
    "text": "2.4 Beyond ggplot2 Themes\nggplot2 comes with eight built-in themes, namely theme_gray(), theme_bw(), theme_classic(), theme_dark(), theme_light(), theme_linedraw(), theme_minimal(), and theme_void().\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n2.4.1 Working with ggtheme Package\nggthemes provides ggplot2 themes that replicate the look of plots by Edward Tufte, Stephen Few, Fivethirtyeight, The Economist, Stata, Excel and The Wall Street Journal, among others.\nIn the example below, The Economist theme was used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n2.4.2 Working with hrbrthemes Package\nhrbrthemes package provides a base theme that focuses on typographic elements, including where various labels are placed as well as the fonts that are used.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\nThe second goal centers around productivity for a production workflow. In fact, this “production workflow” is the context for where the elements of hrbrthemes should be used. Consult this vignette to learn more.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the code chunk above?\n\n\n\n\naxis_title_size argument is used to increase the font size of the axis title to 18,\nbase_size argument is used to increase the default axis label to 15, and\ngrid argument is used to remove the x-axis grid lines."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#beyond-single-graph",
    "href": "Hands_On_Exercise/Hands_On_Ex02/Hands_On_Ex02.html#beyond-single-graph",
    "title": "ISSS608-VAA",
    "section": "2.5 Beyond Single Graph",
    "text": "2.5 Beyond Single Graph\nIt is not unusual that multiple graphs are required to tell a compelling visual story. There are several ggplot2 extensions with functions to compose figures with multiple graphs.\nFirst, three statistical graphics are created using the code chunk below.\n\nCode 1Plot 1Code 2Plot 2Code 3Plot 3\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\nprint(p1)\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\nprint(p3)\n\n\n\n\n\n\n\n\n2.5.1 Creating Composite Graphics using patchwork\npatchwork is a ggplot2 extension specifically designed for combining separate ggplot2 graphs into a single figure.\nThe patchwork package has a simple syntax to create layouts easily. Here’s the general syntax that combines: - Two-Column Layout using the plus sign + - Subgroup Plot using parenthesis () - Two-Row Layout using division sign /\n\n\n2.5.2 Combining Two ggplot2 Graphs\nFigure in the tabset below shows a composite of two histograms created using patchwork. Note how simple the syntax used to create the plot!\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np1 + p2\n\n\n\n\n\n\n2.5.3 Combining Three ggplot2 Graphs\nWe can plot more complex composite by using appropriate operators. For example, the composite figure below is plotted by using:\n\n“|” operator to stack two ggplot2 graphs,\n“/” operator to place the plots beside each other,\n“()” operator the define the sequence of the plotting.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\nTo learn more, refer to Plot Assembly.\n\n\n2.5.4 Creating Composite Figures with Tags\nIn order to identify subplots in text, patchwork also provides auto-tagging capabilities as shown below.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n2.5.5 Creating Figure with Insert\nBeside providing functions to place plots next to each other based on the provided layout. With inset_element() of patchwork, we can place one or several plots or graphic elements freely on top or below another plot.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n2.5.6 Creating Composite Figure using patchwork and ggtheme\nFigure below is created by combining patchwork and theme_economist() of ggthemes package discussed earlier.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html",
    "href": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html",
    "title": "Hands-on-Exercise-1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are installed, they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"Data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html#getting-started",
    "title": "Hands-on-Exercise-1",
    "section": "",
    "text": "The code chunk below uses p_load() of pacman package to check if tidyverse packages are installed in the computer. If they are installed, they will be launched into R.\n\npacman::p_load(tidyverse)\n\n\n\n\n\nexam_data &lt;- read_csv(\"Data/Exam_data.csv\")\n\nRows: 322 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): ID, CLASS, GENDER, RACE\ndbl (3): ENGLISH, MATHS, SCIENCE\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html#using-ggplot2",
    "href": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html#using-ggplot2",
    "title": "Hands-on-Exercise-1",
    "section": "Using ggplot2",
    "text": "Using ggplot2\n\ngeom_point for drawing individual points (e.g., scatter plot)\ngeom_line for drawing lines (e.g., line charts)\ngeom_smooth for drawing smoothed lines (e.g., for simple trends or approximations)\ngeom_bar for drawing bars (e.g., bar charts)\ngeom_histogram for drawing binned values (e.g., histogram)\ngeom_polygon for drawing arbitrary shapes\ngeom_map for drawing polygons in the shape of a map by accessing the data to use using the map_data() function\n\nFor the complete list, please refer here.\n\nPlotting Bar Charts\nThe code chunk below plots a bar chart using geom_bar().\n\nggplot(exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\nPlotting Geometric Objects\nIn a dot plot, the width of a dot corresponds to the bin width (or maximum width, depending on the binning algorithm), and dots are stacked, with each dot representing one observation.\nIn the code chunk below, geom_dotplot() of ggplot2 is used to plot a dot plot.\n\nggplot(exam_data, \n       aes(x=MATHS)) + \n  geom_dotplot(dotsize=0.5)\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nBe warned\n\n\n\nThe y-scale is rather misleading.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code chunk below performs the following two steps:\n\nscale_y_continuous() is used to turn off the y-axis\nbinwidth argument is used to change the binwidth to 2.5.\n\n\n\n\nggplot(exam_data,\n       aes(x=MATHS)) + \n  geom_dotplot(binwidth=2.5, \n               dotsize=0.5) + \n  scale_y_continuous(NULL, \n                     breaks=NULL)\n\n\n\n\n\n\nPlotting Histograms\nIn the code chunk below, geom_histogram() is used to create a simple histogram by using values in MATHS field of exam_data.\n\nggplot(exam_data, \n       aes(x=MATHS)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that the default bin is 30.\n\n\n\n\nModifying Plots by Changing geom()\nIn the code chunk below,\n\nbins argument is used to change the number of bins to 20,\nfill argument is used to shade the histogram with light blue colour, and\ncolor argument is used to change the outline colour of the bars in black\n\n\nggplot(exam_data,\n       aes(x=MATHS)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\")\n\n\n\n\n\n\nModifying Plots by Changing aes()\nThe code chunk below changes the fill of the histogram by using sub-group of aesthetic().\n\nggplot(exam_data,\n       aes(x=MATHS, \n           fill=GENDER)) + \n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to colour, fill and alpha of the geometric.\n\n\n\n\nPlotting Kernel Density Lines\ngeom-density() computes and plots kernel density estimate, which is a smoothed version of the histogram. It is a useful alternative to the histogram for continuous data that comes from an underlying smooth distribution.\nThe code below plots the distribution of Math scores in a kernel density estimate plot.\n\nggplot(exam_data, \n       aes(x=MATHS)) + \n  geom_density()\n\n\n\n\nThe code chunk below plots two kernel density lines by using color or fill arguments of aes().\n\nggplot(exam_data, \n       aes(x=MATHS,\n           color=GENDER)) + \n  geom_density()\n\n\n\n\n\n\nPlotting Boxplots\ngeom_boxplot() displays continuous value lists. It visualises five summary statistics (namely median, two hinges and two whiskers) and all individual outlying points.\nThe code chunk below plots boxplots using geom_boxplot().\n\nggplot(exam_data,\n       aes(y=MATHS,\n           x=GENDER)) + \n  geom_boxplot()\n\n\n\n\nNotches are used in boxplots to visually assess whether the medians of distributions differ. If the notches do not overlap, this is evidence that the medians are different. The code chunk below plots the distribution of Maths scores by gender in notched plots instead of boxplots.\n\nggplot(exam_data,\n       aes(y=MATHS,\n           x=GENDER)) + \n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\nPlotting Violin Plots\ngeom_violin() is designed for creating violin plots. Violin plots are used to compare multiple data distributions. With ordinary density curves, it is difficult to compare more than just a few distributions as the lines visually interfere with each other. With a violin plot, it is easier to compare several distributions as they are placed side by side.\nThe code below plots the distribution of Math scores by gender in a violin plot.\n\nggplot(exam_data,\n       aes(y=MATHS,\n           x=GENDER)) + \n  geom_violin()\n\n\n\n\n\n\nPlotting Scatterplots\ngeom_point() is especially useful for creating scatterplots. The code chunk below plots a scatterplot showing the Math and English grades of pupils using geom_point().\n\nggplot(exam_data,\n       aes(x=MATHS,\n           y=ENGLISH)) +\n  geom_point()\n\n\n\n\n\n\nCombining geom Objects\nThe code chunk below plots the data points on the boxplot by using both geom_boxplot() and geom_point().\n\nggplot(exam_data,\n       aes(y=MATHS,\n           x=GENDER)) + \n  geom_boxplot() + \n  geom_point(position=\"jitter\", \n             size=0.5)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html#adding-summary-statistics-into-plots",
    "href": "Hands_On_Exercise/Hands_On_Ex01/Hands_On_Ex01.html#adding-summary-statistics-into-plots",
    "title": "Hands-on-Exercise-1",
    "section": "Adding Summary Statistics into Plots",
    "text": "Adding Summary Statistics into Plots\nThe Statistic functions statistically transform data, usually as some form of summary. For example:\n\nFrequency of values of a variable (bar graph)\n\na mean\na confidence limit\n\nThere are two ways to use these functions:\n\nadd a stat_() function and override the default geom, or\nadd a geom_() function and override the default stat.\n\n\n\nWorking with stat() - stat_summary() method\nThe code chunk below adds mean values by using stat_summary() function and overrides the default geom.\n\nggplot(exam_data,\n       aes(y=MATHS, x=GENDER)) +\n  geom_boxplot() + \n  stat_summary(geom=\"point\", \n               fun.y=\"mean\", \n               color=\"red\", \n               size=4)\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\nWorking with stat() - geom() method\nThe code chunk below adds mean values by using geom() function and overrides the default stat.\n\nggplot(exam_data,\n       aes(y=MATHS, \n           x=GENDER)) + \n  geom_boxplot() + \n  geom_point(stat=\"summary\", \n             fun.y=\"mean\", \n             color=\"red\", \n             size=4)\n\nWarning in geom_point(stat = \"summary\", fun.y = \"mean\", color = \"red\", size =\n4): Ignoring unknown parameters: `fun.y`\n\n\nNo summary function supplied, defaulting to `mean_se()`\n\n\n\n\n\n\n\nAdding a Best Fit Curve on Scatterplots\nThe interpretability of the scatterplot of English against Math grades of pupils can be improved by adding a best fit curve. In the code chunk below, geom_smooth() is used to plot a best fit curve on the scatterplot.\n\nggplot(exam_data, \n       aes(x=MATHS,\n           y=ENGLISH)) + \n  geom_point() + \n  geom_smooth(size=0.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used is loess."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html",
    "href": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html",
    "title": "Prototyping Modules for Visual Analytics Shiny Application",
    "section": "",
    "text": "Prototyping plays a pivotal role in the development of applications, serving as a foundation step that can significantly impact the success of the final product. By creating prototypes, we can visualise and refine our ideas, and identify potential flaws early. Prototyping can thus help mitigate risks, save time and resources and ensure that the final application aligns with the intended goals and requirements.\nIn this exercise, we will be developing the prototype for our Visual Analytics Shiny Application on Singapore’s climate. Specifically, we will be focusing on the prototype for the Historical Analysis module. This module seeks to visualise Singapore’s historical climate data (including rainfall, temperature and wind speed) through two tabs - Overview and Geographical."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#overview",
    "href": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#overview",
    "title": "Prototyping Modules for Visual Analytics Shiny Application",
    "section": "",
    "text": "Prototyping plays a pivotal role in the development of applications, serving as a foundation step that can significantly impact the success of the final product. By creating prototypes, we can visualise and refine our ideas, and identify potential flaws early. Prototyping can thus help mitigate risks, save time and resources and ensure that the final application aligns with the intended goals and requirements.\nIn this exercise, we will be developing the prototype for our Visual Analytics Shiny Application on Singapore’s climate. Specifically, we will be focusing on the prototype for the Historical Analysis module. This module seeks to visualise Singapore’s historical climate data (including rainfall, temperature and wind speed) through two tabs - Overview and Geographical."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#importing-relevant-r-packages",
    "href": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#importing-relevant-r-packages",
    "title": "Prototyping Modules for Visual Analytics Shiny Application",
    "section": "2.0 Importing Relevant R Packages",
    "text": "2.0 Importing Relevant R Packages\nThe relevant R packages to be used are as follows:\n\ndplyr - for manipulating data\ntidyverse - for tidying data\nggplot2 - for creating plots from data\nplotly - for introducing interactivity into ggplot2 figures\n\n\npacman::p_load(dplyr, tidyverse, fuzzyjoin,\n               ggplot2, plotly, tmap, leaflet, RColorBrewer, htmltools, gganimate, gifski)"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#data-preparation",
    "href": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#data-preparation",
    "title": "Prototyping Modules for Visual Analytics Shiny Application",
    "section": "3.0 Data Preparation",
    "text": "3.0 Data Preparation\n\n3.1 Importing Data\nHistorical daily weather data was retrieved from the Meteological Service Singapore. It consists of data from January 2014 to December 2023 across 12 weather stations. The time period and weather stations were chosen to ensure completeness of the data across all attributes by referencing the station records provided by MSS.\nThe code chunk below downloads the weather data files for the chosen weather stations (S values) and time (yyyymm values) by fitting the two values into the base URL of ‘http://www.weather.gov.sg/files/dailydata/DAILYDATA_’. The tryCatch() function is used to handle errors that may occur during the download process. Specifically, if the file for certain combinations of weather stations and year is not found, it prints a message indicating that the file was not found and continues to the next file.\n\n\nCode to scrape data from chosen weather stations from Jan 2014 to Dec 2023\n# Define the base URL\nbase_url &lt;- \"http://www.weather.gov.sg/files/dailydata/DAILYDATA_\"\n\n# Define the range of S values (weather stations)\ns_values &lt;- c(\"S24\", \"S50\", \"S60\", \"S107\", \"S111\", \"S116\", \"S121\", \"S43\", \"S109\",\n              \"S104\", \"S106\", \"S107\", \"S115\", \"S117\")  \n\n# Define the range of yyyymm values (from 201401 to 202312)\nyyyymm_values &lt;- c(\"201401\", \"201402\", \"201403\", \"201404\", \"201405\", \"201406\", \"201407\", \n                   \"201408\", \"201409\", \"201410\", \"201411\", \"201412\",\n                   \"201501\", \"201502\", \"201503\", \"201504\", \"201505\", \"201506\", \"201507\", \n                   \"201508\", \"201509\", \"201510\", \"201511\", \"201512\",\n                   \"201601\", \"201602\", \"201603\", \"201604\", \"201605\", \"201606\", \"201607\", \n                   \"201608\", \"201609\", \"201610\", \"201611\", \"201612\",\n                   \"201701\", \"201702\", \"201703\", \"201704\", \"201705\", \"201706\", \"201707\", \n                   \"201708\", \"201709\", \"201710\", \"201711\", \"201712\",\n                   \"201801\", \"201802\", \"201803\", \"201804\", \"201805\", \"201806\", \"201807\", \n                   \"201808\", \"201809\", \"201810\", \"201811\", \"201812\",\n                   \"201901\", \"201902\", \"201903\", \"201904\", \"201905\", \"201906\", \"201907\", \n                   \"201908\", \"201909\", \"201910\", \"201911\", \"201912\",\n                   \"202001\", \"202002\", \"202003\", \"202004\", \"202005\", \"202006\", \"202007\", \n                   \"202008\", \"202009\", \"202010\", \"202011\", \"202012\",\n                   \"202101\", \"202102\", \"202103\", \"202104\", \"202105\", \"202106\", \"202107\", \n                   \"202108\", \"202109\", \"202110\", \"202111\", \"202112\",\n                   \"202201\", \"202202\", \"202203\", \"202204\", \"202205\", \"202206\", \"202207\", \n                   \"202208\", \"202209\", \"202210\", \"202211\", \"202212\",\n                   \"202301\", \"202302\", \"202303\", \"202304\", \"202305\", \"202306\", \"202307\", \n                   \"202308\", \"202309\", \"202310\", \"202311\", \"202312\") \n\n# Specify the folder to save the files\nfolder &lt;- \"Data/\"\n\n# Loop through each combination of s and yyyymm\nfor (s in s_values) {\n  for (yyyymm in yyyymm_values) {\n    # Construct the URL\n    url &lt;- paste0(base_url, s, \"_\", yyyymm, \".csv\")\n    \n    # Construct the file name with folder path\n    filename &lt;- paste0(folder, \"DAILYDATA_\", s, \"_\", yyyymm, \".csv\")\n    \n    # Download the file with error handling\n    tryCatch({\n      download.file(url, destfile = filename, mode = \"wb\")\n      cat(\"Downloaded\", filename, \"\\n\")\n    }, error = function(e) {\n      cat(\"File not found:\", filename, \"\\n\")\n    })\n  }\n}\n\n\nThe code chunk below reads all the CSV files saved in the previous step and combines them into a single data frame df. The resultant dataframe was saved as an .rds file format using write_rds().\n\n\nCode to combine datasets into one\n# Get list of files in the directory\nfile_list &lt;- list.files(\"Data/WeatherData\", pattern=\".csv\", full.names = TRUE)\n\n# Initialize an empty list to store preprocessed data frames\npreprocessed_data_frames &lt;- list()\n\n# Loop through each file\nfor (file in file_list) {\n  # Read the file and remove header row\n  data &lt;- read_csv(file, locale(encoding=\"UTF-8\"), show_col_types = FALSE)[-1, ]\n  \n  # Replace \"\\x97\" (special character to indicate no measurements done) with NA \n  data &lt;- data.frame(lapply(data, function(x) str_replace_all(x, fixed(\"\\x97\"), NA)))\n\n  # Replace \"ó\" (special character to indicate no measurements done) with NA \n  data &lt;- data.frame(lapply(data, function(x) str_replace_all(x, fixed(\"ó\"), NA)))\n  \n  # Replace \"-\" (special character to indicate no measurements done) with NA \n  data &lt;- data.frame(lapply(data, function(x) str_replace_all(x, fixed(\"-\"), NA)))\n  \n  # Store the preprocessed data frame in the list\n  preprocessed_data_frames[[length(preprocessed_data_frames) + 1]] &lt;- data\n}\n\n# Ensure all data frames have the same column names and order\ncombined_df &lt;- do.call(rbind, preprocessed_data_frames)\n\n# Naming columns\ncolnames(combined_df) &lt;- c(\"Station\", \"Year\", \"Month\", \"Day\", \"Total Daily Rainfall\", \"Highest 30 Min Rainfall\", \"Highest 60 Min Rainfall\", \"Highest 120 Min Rainfall\", \"Mean Temperature\", \"Maximum Temperature\", \"Minimum Temperature\", \"Mean Wind Speed\", \"Max Wind Speed\")\n\n# Save combined_df as .rds file format\nwrite_rds(combined_df, \"Data/combined_df.rds\")\n\n\nThe read_rds() function was then used to read the R object.\n\ndf&lt;- read_rds(\"Data/combined_df.rds\")\n\n\n\n3.2 Data Cleaning\nThe following data cleaning steps were implemented on the data:\n\nHandling Missing Values: Using complete.cases() to check the rows with complete data, there are 2599 rows with NA values. Since our data has over 47,000 records and incomplete rows form only about 5% of the data, we will drop the rows with NA values for simplicity.\nDealing with Duplicates: combined_df[duplicated(combined_df), ] returned a data frame with zero rows, indicating that there are no duplicate rows. No further action was thus needed.\nData Type Conversion: All columns are of character type. All columns except the ‘Station’ column was converted to its appropriate data type using as.numeric().\n\n\n\nCode to clean data\n# Checking for NA records and dropping rows with NA records\ncat('There are ', sum(!complete.cases(df)), 'rows with NA values')\n\n\nThere are  2599 rows with NA values\n\n\nCode to clean data\ndf &lt;- na.omit(df)\n\n# Checking for duplicate records\n# df[duplicated(df), ]\n\n# Converting all columns to numeric form except first\ndf[, -1] &lt;- lapply(df[, -1], as.numeric)"
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#data-visualisation",
    "href": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#data-visualisation",
    "title": "Prototyping Modules for Visual Analytics Shiny Application",
    "section": "4.0 Data Visualisation",
    "text": "4.0 Data Visualisation\nFor simplicity, we will use yearly intervals, all regions and the whole period from 2014 to 2023 for this section. The parameters of this visualisation to allow users to explore the data and customise the visualisation will be discussed later in Section 5.1.\n\n4.1 Overview of Historical Daily Weather Data\nIn this section, we will be developing the prototype for an overview of the historical daily weather data.\n\n\nCode to process data for plotting\n# Selecting relevant columns\ndf1 &lt;- df[, c(2:5, 9:11)] %&gt;%\n  \n  # Aggregating data using yearly intervals\n  group_by(Year) %&gt;%\n  summarise(mean_temp = mean(`Mean Temperature`), \n            mean_maxtemp = mean(`Maximum Temperature`), \n            mean_mintemp = mean(`Minimum Temperature`), \n            mean_rainfall = mean(`Total Daily Rainfall`))\n\n\n\n\n\n\n\n\nDesign Considerations\n\n\n\n\nColour Differentiation: Distinct colours were used for the bars and lines to differentiate between the various rainfall and temperature data for greater clarity.\nVisual Hierachy: More critical data points (temperature in this case) were emphasised by placing the temperature line plots above the rainfall bar charts. This helps viewers focus on the primary temperature information while still being able to interpret the secondary rainfall data.\nAxis Labels and Units: The x-axis and y-axis were clearly labelled with its corresponding units to facilitate understanding.\nGridlines and Ticks: Vertical gridlines were retained to aid in interpretation and aligning data points. However, horizontal grid lines were removed as it clutters the chart and do not add significant value since the bars and line markers clearly denote the year to which the data point belong.\nLegend: A legend was included to explain the meaning of each element in the chart.\n\n\n\n\n\nCode for plotting\nfig &lt;- \n  plot_ly(data=df1, \n          x=~Year, # Set x=Year for yearly intervals\n          \n            # Plotting line charts for temperature data\n          y=~mean_temp, \n            type='scatter', mode='lines+markers',\n            marker=list(color=\"#008000\", size=5, line=list(color=\"#008000\", width=1.5)), \n            line=list(color=\"#008000\", width=1.5),\n            yaxis=\"y\", # Specifying which axis to use\n            name=\"Mean Temperature\") %&gt;% \n  \n    add_trace(y=~mean_maxtemp, \n            type='scatter', mode='lines+markers',\n            marker=list(color=\"#8B0000\", size=5, line=list(color=\"#8B0000\", width=1.5)), \n            line=list(color=\"#8B0000\", width=1.5),\n            yaxis=\"y\", # Specifying which axis to use\n            name=\"Maximum Temperature\") %&gt;% \n  \n    add_trace(y=~mean_mintemp, \n            type='scatter', mode='lines+markers',\n            marker=list(color=\"#02367B\", size=5, line=list(color=\"#02367B\", width=1.5)), \n            line=list(color=\"#02367B\", width=1.5),\n            yaxis=\"y\", # Specifying which axis to use\n            name=\"Minimum Temperature\") %&gt;%\n  \n  add_trace(y=~mean_rainfall, \n          type='bar', \n          marker=list(color=\"lightblue\", line=list(color=\"black\", width=1)),\n          yaxis=\"y2\", # Specifying which axis to use\n          name=\"Total Daily Rainfall\") %&gt;% \n  \n  layout(\n    title=\"Average Yearly Temperature and Rainfall Over the Years\", \n    \n    # Setting up x-axis \n    xaxis = list(title=\"Year\",\n                 tickmode=\"linear\"), # Showing all tick marks\n    \n    # Setting up primary y-axis\n    yaxis = list(overlaying=\"y2\",  # Set line plots to overlay bar chart\n                 title=\"Average Yearly Temperature (°C)\"), \n    \n    # Setting up secondary axis\n    yaxis2 = list(side=\"right\",\n                  title=\"Average Yearly Rainfall (mm)\",\n                  automargin=TRUE),\n    \n    # Formatting legend\n    legend = list(orientation='h', # Orientating legend\n                  y=-0.2))   # Positioning legend to bottom of plot\n\nfig\n\n\n\n\n\n\n\n\n4.2 Geographical Overview of Historical Daily Weather Data\nIn this section, we will be developing the prototype for a geographical overview of the historical daily weather data.\nThe geospatial data of the geographical boundary of Singapore at the planning subzone level is provided by data.gov.sg.\n\n\nCode to process data for plotting\n# Importing geospatial data\nmpsz &lt;- st_read(dsn = \"Data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n# Selecting relevant columns\ndf2 &lt;- df[, c(1:4, 5, 9)] %&gt;%\n  # Aggregating data using yearly intervals\n  group_by(Station, Year) %&gt;%\n  summarise(mean_rainfall = mean(`Total Daily Rainfall`),\n            mean_temp = mean(`Mean Temperature`)) %&gt;%\n  ungroup()\n\n# Joining attribute data and geospatial data\ndf2 &lt;- df2 %&gt;% mutate(Station = toupper(Station)) # Converting to uppercase\n# Join based on SUBZONE_N = Station\nmpsz_df2 &lt;- left_join(mpsz, df2, \n                      by=c(\"PLN_AREA_N\"=\"Station\" )) \n\nwrite_rds(mpsz_df2, \"Data/mpsz_ds2.rds\")\n\n\n\nTemperatureRainfall\n\n\n\n\nCode for plotting static map for temperature\n# Reading data\nmpsz_df2 &lt;- read_rds(\"Data/mpsz_ds2.rds\")\n\n# Activating viewing mode\ntmap_mode(\"plot\")\n\n# Plotting data by geography\nmap_static_temp &lt;- \n  tm_shape(mpsz_df2) + \n    \n    # Adding data points onto map\n    tm_fill(\"mean_temp\", \n            style=\"pretty\", # Display many colours over continuous palette\n            palette=\"YlOrRd\", # Using yellow, orange, red palette for temperature data\n            title = \"Mean Temperature (°C)\") +\n    \n    # Labelling plot\n    tm_layout(main.title=\"Mean Temperature by Planning Area\", \n              main.title.position=\"center\",\n              main.title.size=1.2,\n              legend.height=0.65,\n              legend.width=0.35,\n              legend.position=c(\"right\", \"bottom\"),\n              frame=TRUE) +\n    \n    tm_borders(alpha=0.5) + \n    \n    # Adding caption to cite sources\n    tm_credits(\"Source: Planning Area Boundary from Urban Redevelopment Authority and Weather Data from Meteological Service Singapore\", \n               position=c(\"left\", \"bottom\"))\n\n\n\n\nCode for plotting animated map for temperature\n# Activating viewing mode\ntmap_mode(\"plot\")\n\nmap_static_temp &lt;- map_static_temp + \n  # Plotting by year\n  tm_facets(by=\"Year\", \n            nrow=1, ncol=1, \n            free.coords=FALSE,\n            drop.NA.facets=TRUE) + # Drop facets with Year = NA \n  \n  # Adding grey polygons for NA values - needed to standardise map across all years\n  tm_shape(mpsz_df2[is.na(mpsz_df2$mean_temp), ]) +\n  tm_polygons(fill = \"grey\")\n\n# Plotting animated map\ntmap_animation(map_static_temp, delay=75,\n               filename=\"map_temp.gif\")\n\n\n\n\n\n\n\nCode for plotting static map for rainfall\n# Activating viewing mode\ntmap_mode(\"plot\")\n\n# Plotting data by geography\nmap_static_rainfall &lt;- \n  tm_shape(mpsz_df2) + \n    \n    # Adding data points onto map\n    tm_fill(\"mean_rainfall\", \n            style=\"pretty\", # Display many colours over continuous palette\n            palette=\"Blues\", # Using blue palette for temperature data\n            title = \"Rainfall (mm)\") +\n    \n    # Labelling plot\n    tm_layout(main.title=\"Mean Total Daily Rainfall by Planning Area\", \n              main.title.position=\"center\",\n              main.title.size=1.2,\n              legend.height=0.65,\n              legend.width=0.35,\n              legend.position=c(\"right\", \"bottom\"),\n              frame=TRUE) +\n    \n    tm_borders(alpha=0.5) + \n    \n    # Adding caption to cite sources\n    tm_credits(\"Source: Planning Area Boundary from Urban Redevelopment Authority and Weather Data from Meteological Service Singapore\", \n               position=c(\"left\", \"bottom\"))\n\n\n\n\nCode for plotting animated map for rainfall\n# Activating viewing mode\ntmap_mode(\"plot\")\n\nmap_static_rainfall &lt;- map_static_rainfall + \n  # Plotting by year\n  tm_facets(by=\"Year\", \n            nrow=1, ncol=1, \n            free.coords=FALSE,\n            drop.NA.facets=TRUE) + # Drop facets with Year = NA \n  \n  # Adding grey polygons for NA values - needed to standardise map across all years\n  tm_shape(mpsz_df2[is.na(mpsz_df2$mean_temp), ]) +\n  tm_polygons(fill = \"grey\")\n\n# Plotting animated map\ntmap_animation(map_static_rainfall, delay=75,\n               filename=\"map_rainfall.gif\")\n\n\n\n\n\n\nAs we only have data for 12 weather stations, the significant portion of grey polygons on the map are due to the missing weather data for the respective weather stations. The data was limited to ensure that the plots could be developed quickly. When the Shiny application is developed, we would expected to have a more complete data of more than 60 weather stations to have a more comprehensive map."
  },
  {
    "objectID": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#user-interface-ui-design-of-shiny-application",
    "href": "Take_Home_Exercise/Take_Home_Ex04/Take_Home_Ex04.html#user-interface-ui-design-of-shiny-application",
    "title": "Prototyping Modules for Visual Analytics Shiny Application",
    "section": "5.0 User-Interface (UI) Design of Shiny Application",
    "text": "5.0 User-Interface (UI) Design of Shiny Application\nIn this module of the application, the user is able to select the weather data from their preferred weather station(s) to analyse and compare the various time series timeline they are interested in. Through this, the user is able to understand the historical trends and average of temperature and/or rainfall data during the selected time period.\n\n5.1 Storyboard of Shiny Application\n\n\n\n5.2 User-Specified Parameters of Shiny Application\nThe figure below shows some of the interactive features of the application. The main objective is to allow users to have an overview of the historical data of their selection weather station(s) and time period.\n\nThe parameters users can specify include the following:\n\nAveraging Period: Users can choose the averaging period they prefer, including monthly, 1-year, 5-years or 10-years, using the drop-down list.\nPeriod for Analysis: Users can choose the start and end date they prefer, from January 1980 to current using the slider. However, limited data is available from January 1980 to December 2013 and may cause the plots to be incomplete.\nWeather Station(s): Users can choose their preferred weather station(s) by checking the checkboxes of weather stations they are interested in. A checkbox for “All weather stations” will be included for convenience.\nUpdate Data: Users can choose to update the data to include the most current data points available on the MSS website. The application is built using data up till December 2023. Users may be interested in analysing more current weather data and can include such points using the “Update Data” button to scrape the data from the MSS website.\nSelection of Map Type: Users can choose to display temperature or rainfall data on the map using the radio buttons available. Users can also choose the type of data to be displayed, such as mean, maximum or minimum, using the drop-down list.\nSelection of Plot: Users can choose the data points they wish to display on the plot by toggling through the legend of the plot."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html",
    "href": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html",
    "title": "Visualising Distribution",
    "section": "",
    "text": "In this chapter, we will explore ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#learning-outcome",
    "title": "Visualising Distribution",
    "section": "",
    "text": "In this chapter, we will explore ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#getting-started",
    "title": "Visualising Distribution",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n1.2.2 Data import\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"Data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#visualising-distribution-with-ridgeline-plot",
    "title": "Visualising Distribution",
    "section": "1.3 Visualising Distribution with Ridgeline Plot",
    "text": "1.3 Visualising Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n\n\n1.3.1 Plotting ridgeline graph: ggridges method\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, aes(x = ENGLISH, y = CLASS)) + geom_density_ridges( scale = 3, rel_min_height = 0.01, bandwidth = 3.4, fill = lighten(\"#7097BB\", .3), color = \"white\" ) + scale_x_continuous( name = \"English grades\", expand = c(0, 0) ) + scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) + theme_ridges()\n\n\n\n\n\n\n1.3.2 Varying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n1.3.3 Mapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nPlotCode\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n1.3.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands_On_Exercise/Hands_On_Ex4A/Hands_On_Ex04A.html#visualising-distribution-with-raincloud-plot",
    "title": "Visualising Distribution",
    "section": "1.4 Visualising Distribution with Raincloud Plot",
    "text": "1.4 Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n1.4.1 Plotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nTip\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\n\n\n\n** 1.4.2 Adding the boxplot with geom_boxplot()**\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n1.4.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n1.4.4 Finishing touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html",
    "href": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html",
    "title": "Visualising Statistical Analysis",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#learning-outcome",
    "title": "Visualising Statistical Analysis",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experience on using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#visual-statistical-analysis-with-ggstatsplot",
    "href": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#visual-statistical-analysis-with-ggstatsplot",
    "title": "Visualising Statistical Analysis",
    "section": "1.2 Visual Statistical Analysis with ggstatsplot",
    "text": "1.2 Visual Statistical Analysis with ggstatsplot\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.\n\nTo provide alternative statistical inference methods by default.\nTo follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting. For example, here are results from a robust t-test:"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#getting-started",
    "title": "Visualising Statistical Analysis",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\n\n1.3.1 Installing and launching R packages\nIn this exercise, ggstatsplot and tidyverse will be used.\n\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n1.3.2 Importing data\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"Data/Exam_data.csv\")\n\n\n\n1.3.3 One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on English scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n1.3.4 Unpacking the Bayes Factor\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nThat’s because the Bayes factor gives us a way to evaluate the data in favor of a null hypothesis, and to use external information to do so. It tells us what the weight of the evidence is in favor of a given hypothesis.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. It can be defined mathematically as\n\n\n\nWhat is the Bayes Factor anyway? - Dan Oehm | Gradient Descending\n\n\nThe Schwarz criterion is one of the easiest ways to calculate rough approximation of the Bayes Factor.\n\n\n\n1.3.5 How to interpret Bayes Factor\nA Bayes Factor can be any positive number. One of the most common interpretations is this one—first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013:\n\n\n\n1.3.6 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n1.3.7 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on English score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n1.3.8 Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and English scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n1.3.9 Significant Test of Association (Depedence) : ggbarstats() methods\nIn the code chunk below, the Maths scores is binned into a 4-class variable by using cut().\n\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\nIn this code chunk below ggbarstats() is used to build a visual for Significant Test of Association\n\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#visualising-models",
    "href": "Hands_On_Exercise/Hands_On_Ex04B/Hands_On_Ex04B.html#visualising-models",
    "title": "Visualising Statistical Analysis",
    "section": "1.4 Visualising Models",
    "text": "1.4 Visualising Models\nIn this section, you will learn how to visualise model diagnostic and model parameters by using parameters package.\n\nToyota Corolla case study will be used. The purpose of study is to build a model to discover factors affecting prices of used-cars by taking into consideration a set of explanatory variables.\n\n\n1.4.1 Installing and loading the required libraries\n\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n1.4.2 Importing Excel file: readxl methods\nIn the code chunk below, read_xls() of readxl package is used to import the data worksheet of ToyotaCorolla.xls workbook into R.\n\ncar_resale &lt;- read_xls(\"Data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nNotice that the output object car_resale is a tibble data frame.\n\n\n1.4.3 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n1.4.4 Model Diagnostic: checking for multicolinearity:\nIn the code chunk, check_collinearity() of performance package.\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n1.4.5 Model Diagnostic: checking normality assumption\nIn the code chunk, check_normality() of performance package.\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n1.4.6 Model Diagnostic: Check model for homogeneity of variances\nIn the code chunk, check_heteroscedasticity() of performance package.\n\ncheck_h &lt;- check_heteroscedasticity(model1)\nplot(check_h)\n\n\n\n\n\n\n1.4.7 Model Diagnostic: Complete check\nWe can also perform the complete by using check_model().\n\ncheck_model(model1)\n\n\n\n\n\n\n1.4.8 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package is used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n1.4.9 Visualising Regression Parameters: ggcoefstats() methods\nIn the code below, ggcoefstats() of ggstatsplot package to visualise the parameters of a regression model.\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html",
    "href": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html",
    "title": "Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#overview",
    "title": "Funnel Plots for Fair Comparisons",
    "section": "",
    "text": "Funnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. By the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#installing-and-launching-r-packages",
    "href": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#installing-and-launching-r-packages",
    "title": "Funnel Plots for Fair Comparisons",
    "section": "1.2 Installing and Launching R Packages",
    "text": "1.2 Installing and Launching R Packages\nIn this exercise, four R packages will be used. They are:\n\nreadr for importing csv into R.\nFunnelPlotR for creating funnel plot.\nggplot2 for creating funnel plot manually.\nknitr for building static html table.\nplotly for creating interactive funnel plot.\n\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#importing-data",
    "href": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#importing-data",
    "title": "Funnel Plots for Fair Comparisons",
    "section": "1.3 Importing Data",
    "text": "1.3 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#funnelplotr-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#funnelplotr-methods",
    "title": "Funnel Plots for Fair Comparisons",
    "section": "1.4 FunnelPlotR methods",
    "text": "1.4 FunnelPlotR methods\nFunnelPlotR package uses ggplot to generate funnel plots. It requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n1.4.1 FunnelPlotR methods: The basic plot\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Positive,\n  denominator = covid19$Death,\n  group = covid19$`Sub-district`\n)\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\ngroup in this function is different from the scatterplot. Here, it defines the level of the points to be plotted i.e. Sub-district, District or City. If Cityc is chosen, there are only six data points.\nBy default, data_typeargument is “SR”.\nlimit: Plot limits, accepted values are: 95 or 99, corresponding to 95% or 99.8% quantiles of the distribution.\n\n\n\n\n\n1.4.2 FunnelPlotR methods: Makeover 1\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\nxrange and yrange are used to set the range of x-axis and y-axis\n\n\n\n\n\n1.4.3 FunnelPlotR methods: Makeover 2\nThe code chunk below plots a funnel plot.\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex04D/Hands_On_Ex04D.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Funnel Plots for Fair Comparisons",
    "section": "1.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "1.5 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nIn this section, you will gain hands-on experience on building funnel plots step-by-step by using ggplot2. It aims to enhance you working experience of ggplot2 to customise speciallised data visualisation like funnel plot.\n\n1.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we need to derive cumulative death rate and standard error of cumulative death rate.\n\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\nNext, the fit.mean is computed by using the code chunk below.\n\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n1.5.2 Calculate lower and upper limits for 95% and 99.9% CI\nThe code chunk below is used to compute the lower and upper limits for 95% confidence interval.\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n1.5.3 Plotting a static funnel plot\nIn the code chunk below, ggplot2 functions are used to plot a static funnel plot.\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n1.5.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html",
    "title": "Visualising Uncertainty",
    "section": "",
    "text": "Visualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#learning-outcome",
    "title": "Visualising Uncertainty",
    "section": "",
    "text": "Visualising uncertainty is relatively new in statistical graphics. In this chapter, you will gain hands-on experience on creating statistical graphics for visualising uncertainty. By the end of this chapter you will be able:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#getting-started",
    "title": "Visualising Uncertainty",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nplotly for creating interactive plot,\ngganimate for creating animation plot,\nDT for displaying interactive html table,\ncrosstalk for for implementing cross-widget interactions (currently, linked brushing and filtering), and\nggdist for visualising distribution and uncertainty.\n\n\ndevtools::install_github(\"wilkelab/ungeviz\", force=TRUE)\n\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n1.2.2 Data import\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualizing-the-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Visualising Uncertainty",
    "section": "1.3 Visualizing the uncertainty of point estimates: ggplot2 methods",
    "text": "1.3 Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n\n\n\n\n\nImportant\n\n\n\n\nDon’t confuse the uncertainty of a point estimate with the variation in the sample\n\n\n\nIn this section, you will learn how to plot error bars of maths scores by race by using data provided in exam tibble data frame.\nFirstly, code chunk below will be used to derive the necessary summary statistics.\n\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\ngroup_by() of dplyr package is used to group the observation by RACE,\nsummarise() is used to compute the count of observations, mean, standard deviation\nmutate() is used to derive standard error of Maths by RACE, and\nthe output is save as a tibble data table called my_sum.\n\n\n\nNext, the code chunk below will be used to display my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n1.3.1 Plotting standard error bars of point estimates\nNow we are ready to plot the standard error bars of mean maths score by race as shown below.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”.\n\n\n\n\n\n\n\n\n1.3.2 Plotting confidence interval of point estimates\nInstead of plotting the standard error bar of point estimates, we can also plot the confidence intervals of mean maths score by race.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nThe confidence intervals are computed by using the formula mean+/-1.96*se.\nThe error bars is sorted by using the average maths scores.\nlabs() argument of ggplot2 is used to change the x-axis label.\n\n\n\n\n\n\n\n\n** 1.3.3 Visualizing the uncertainty of point estimates with interactive error bars**\nIn this section, you will learn how to plot interactive error bars for the 99% confidence interval of mean maths score by race as shown in the figure below.\n\n\nCode\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualising-uncertainty-ggdist-package",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualising-uncertainty-ggdist-package",
    "title": "Visualising Uncertainty",
    "section": "11.4 Visualising Uncertainty: ggdist package",
    "text": "11.4 Visualising Uncertainty: ggdist package\nggdist is an R package that provides a flexible set of ggplot2 geoms and stats designed especially for visualising distributions and uncertainty.\nIt is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n11.4.1 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo read the syntax reference for more detail as this function comes with many arguments\n\n\nFor example, in the code chunk below the following arguments are used:\n\n.width = 0.95\n.point = median\n.interval = qi\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n1.4.3 Visualizing the uncertainty of point estimates: ggdist methods\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"skyblue\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Visualising Uncertainty",
    "section": "1.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "1.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nlibrary(ungeviz)\n\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#0072B2\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#D55E00\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "href": "Hands_On_Exercise/Hands_On_Ex04C/Hands_On_Ex04C.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops-1",
    "title": "Visualising Uncertainty",
    "section": "1.6 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "1.6 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), \n            y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, \n    width = 0.05), \n    size = 0.4, \n    color = \"#0072B2\", \n    alpha = 1/2) +\n  geom_hpline(data = sampler(25, \n                             group = RACE), \n              height = 0.6, \n              color = \"#D55E00\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html",
    "href": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html",
    "title": "Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) Its display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#overview",
    "title": "Creating Ternary Plot with R",
    "section": "",
    "text": "Ternary plots are a way of displaying the distribution and variability of three-part compositional data. (For example, the proportion of aged, economy active and young population or sand, silt, and clay in soil.) Its display is a triangle with sides scaled from 0 to 1. Each side represents one of the three components. A point is plotted so that a line drawn perpendicular from the point to each leg of the triangle intersect at the component values of the point.\nIn this hands-on, you will learn how to build ternary plot programmatically using R for visualising and analysing population structure of Singapore.\nThe hands-on exercise consists of four steps:\n\nInstall and launch tidyverse and ggtern packages.\nDerive three new measures using mutate() function of dplyr package.\nBuild a static ternary plot using ggtern() function of ggtern package.\nBuild an interactive ternary plot using plot-ly() function of Plotly R package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#installing-and-launching-r-packages",
    "href": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#installing-and-launching-r-packages",
    "title": "Creating Ternary Plot with R",
    "section": "1.2 Installing and launching R packages",
    "text": "1.2 Installing and launching R packages\nFor this exercise, two main R packages will be used in this hands-on exercise, they are:\n\nggtern, a ggplot extension specially designed to plot ternary diagrams. The package will be used to plot static ternary plots.\nPlotly R, an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js . The plotly R libary contains the ggplotly function, which will convert ggplot2 figures into a Plotly object.\n\nWe will also need to ensure that selected tidyverse family packages namely: readr, dplyr and tidyr are also installed and loaded.\nIn this exercise, version 3.2.1 of ggplot2 will be installed instead of the latest version of ggplot2. This is because the current version of ggtern package is not compatible to the latest version of ggplot2.\nThe code chunks below will accomplish the task.\n\npacman::p_load(plotly, ggtern, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#data-preparation",
    "href": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#data-preparation",
    "title": "Creating Ternary Plot with R",
    "section": "1.3 Data Preparation",
    "text": "1.3 Data Preparation\n\n1.3.1 The data\nFor the purpose of this hands-on exercise, the Singapore Residents by Planning AreaSubzone, Age Group, Sex and Type of Dwelling, June 2000-2018 data will be used. The data set has been downloaded and included in the data sub-folder of the hands-on exercise folder. It is called respopagsex2000to2018_tidy.csv and is in csv file format.\n\n\n1.3.2 Importing Data\nTo important respopagsex2000to2018_tidy.csv into R, read_csv() function of readr package will be used.\n\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n1.3.3 Preparing the Data\nNext, use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#plotting-ternary-diagram-with-r",
    "href": "Hands_On_Exercise/Hands_On_Ex05A/Hands_On_Ex05A.html#plotting-ternary-diagram-with-r",
    "title": "Creating Ternary Plot with R",
    "section": "1.4 Plotting Ternary Diagram with R",
    "text": "1.4 Plotting Ternary Diagram with R\n\n1.4.1 4.1 Plotting a static ternary diagram\nUse ggtern() function of ggtern package to create a simple ternary plot.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n#Building the static ternary plot \nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) + \n  geom_point() + \n  labs(title=\"Population structure, 2015\") + \n  theme_rgbw()\n\n\n\n\n1.4.2 Plotting an interative ternary diagram\nThe code below create an interactive ternary plot using plot_ly() function of Plotly R.\n\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html",
    "href": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html",
    "title": "Visualising Distribution",
    "section": "",
    "text": "In this chapter, we will explore ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#learning-outcome",
    "title": "Visualising Distribution",
    "section": "",
    "text": "In this chapter, we will explore ridgeline plot and raincloud plot by using ggplot2 and its extensions."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#getting-started",
    "title": "Visualising Distribution",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Installing and loading the packages\nFor the purpose of this exercise, the following R packages will be used, they are:\n\ntidyverse, a family of R packages for data science process,\nggridges, a ggplot2 extension specially designed for plotting ridgeline plots, and\nggdist for visualising distribution and uncertainty.\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n1.2.2 Data import\nFor the purpose of this exercise, Exam_data.csv will be used.\n\nexam &lt;- read_csv(\"Data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#visualising-distribution-with-ridgeline-plot",
    "title": "Visualising Distribution",
    "section": "1.3 Visualising Distribution with Ridgeline Plot",
    "text": "1.3 Visualising Distribution with Ridgeline Plot\nRidgeline plot (sometimes called Joyplot) is a data visualisation technique for revealing the distribution of a numeric value for several groups. Distribution can be represented using histograms or density plots, all aligned to the same horizontal scale and presented with a slight overlap.\nFigure below is a ridgelines plot showing the distribution of English score by class.\n\n\n\n\n\n\n\nNote\n\n\n\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.\n\n\n\n\n1.3.1 Plotting ridgeline graph: ggridges method\nThere are several ways to plot ridgeline plot with R. In this section, you will learn how to plot ridgeline plot by using ggridges package.\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nThe ridgeline plot below is plotted by using geom_density_ridges().\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, aes(x = ENGLISH, y = CLASS)) + geom_density_ridges( scale = 3, rel_min_height = 0.01, bandwidth = 3.4, fill = lighten(\"#7097BB\", .3), color = \"white\" ) + scale_x_continuous( name = \"English grades\", expand = c(0, 0) ) + scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) + theme_ridges()\n\n\n\n\n\n\n1.3.2 Varying fill colors along the x axis\nSometimes we would like to have the area under a ridgeline not filled with a single solid color but rather with colors that vary in some form along the x axis. This effect can be achieved by using either geom_ridgeline_gradient() or geom_density_ridges_gradient(). Both geoms work just like geom_ridgeline() and geom_density_ridges(), except that they allow for varying fill colors. However, they do not allow for alpha transparency in the fill. For technical reasons, we can have changing fill colors or transparency but not both.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n1.3.3 Mapping the probabilities directly onto colour\nBeside providing additional geom objects to support the need to plot ridgeline plot, ggridges package also provides a stat function called stat_density_ridges() that replaces stat_density() of ggplot2.\nFigure below is plotted by mapping the probabilities calculated by using stat(ecdf) which represent the empirical cumulative density function for the distribution of English score.\n\nPlotCode\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important include the argument calc_ecdf = TRUE in stat_density_ridges().\n\n\n\n\n1.3.4 Ridgeline plots with quantile lines\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown in the figure below.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands_On_Exercise/Hands_On_Ex04A/Hands_On_Ex04A.html#visualising-distribution-with-raincloud-plot",
    "title": "Visualising Distribution",
    "section": "1.4 Visualising Distribution with Raincloud Plot",
    "text": "1.4 Visualising Distribution with Raincloud Plot\nRaincloud Plot is a data visualisation techniques that produces a half-density to a distribution plot. It gets the name because the density plot is in the shape of a “raincloud”. The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). The boxplot does not show where densities are clustered, but the raincloud plot does!\nIn this section, you will learn how to create a raincloud plot to visualise the distribution of English score by race. It will be created by using functions provided by ggdist and ggplot2 packages.\n\n1.4.1 Plotting a Half Eye graph\nFirst, we will plot a Half-Eye graph by using stat_halfeye() of ggdist package.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\nTip\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\n\n\n\n** 1.4.2 Adding the boxplot with geom_boxplot()**\nNext, we will add the second geometry layer using geom_boxplot() of ggplot2. This produces a narrow boxplot. We reduce the width and adjust the opacity.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n1.4.3 Adding the Dot Plots with stat_dots()\nNext, we will add the third geometry layer using stat_dots() of ggdist package. This produces a half-dotplot, which is similar to a histogram that indicates the number of samples (number of dots) in each bin. We select side = “left” to indicate we want it on the left-hand side.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n1.4.4 Finishing touch\nLastly, coord_flip() of ggplot2 package will be used to flip the raincloud chart horizontally to give it the raincloud appearance. At the same time, theme_economist() of ggthemes package is used to give the raincloud chart a professional publishing standard look.\n\nPlotCode Chunk\n\n\n\n\n\n\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html",
    "title": "Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#overview",
    "title": "Visual Correlation Analysis",
    "section": "",
    "text": "Correlation coefficient is a popular statistic that use to measure the type and strength of the relationship between two variables. The values of a correlation coefficient ranges between -1.0 and 1.0. A correlation coefficient of 1 shows a perfect linear relationship between the two variables, while a -1.0 shows a perfect inverse relationship between the two variables. A correlation coefficient of 0.0 shows no linear relationship between the two variables.\nWhen multivariate data are used, the correlation coefficeints of the pair comparisons are displayed in a table form known as correlation matrix or scatterplot matrix.\nThere are three broad reasons for computing a correlation matrix.\n\nTo reveal the relationship between high-dimensional variables pair-wisely.\nTo input into other analyses. For example, people commonly use correlation matrices as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise.\nAs a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression’s estimates will be unreliable.\n\nWhen the data is large, both in terms of the number of observations and the number of variables, Corrgram tend to be used to visually explore and analyse the structure and the patterns of relations among variables. It is designed based on two main schemes:\n\nRendering the value of a correlation to depict its sign and magnitude, and\nReordering the variables in a correlation matrix so that “similar” variables are positioned adjacently, facilitating perception.\n\nIn this hands-on exercise, you will learn how to plot data visualisation for visualising correlation matrix with R. It consists of three main sections. First, you will learn how to create correlation matrix using pairs() of R Graphics. Next, you will learn how to plot corrgram using corrplot package of R. Lastly, you will learn how to create an interactive correlation matrix using plotly R."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#installing-and-launching-r-packages",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#installing-and-launching-r-packages",
    "title": "Visual Correlation Analysis",
    "section": "1.2 Installing and Launching R Packages",
    "text": "1.2 Installing and Launching R Packages\nBefore you get started, you are required to open a new Quarto document. Keep the default html authoring format.\nNext, you will use the code chunk below to install and launch corrplot, ggpubr, plotly and tidyverse in RStudio.\n\npacman::p_load(corrplot, ggstatsplot, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#importing-and-preparing-the-data-set",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#importing-and-preparing-the-data-set",
    "title": "Visual Correlation Analysis",
    "section": "1.3 Importing and Preparing The Data Set",
    "text": "1.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the Wine Quality Data Set of UCI Machine Learning Repository will be used. The data set consists of 13 variables and 6497 observations. For the purpose of this exercise, we have combined the red wine and white wine data into one data file. It is called wine_quality and is in csv file format.\n\n1.3.1 Importing Data\nFirst, let us import the data into R by using read_csv() of readr package.\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\nNotice that beside quality and type, the rest of the variables are numerical and continuous data type."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#building-correlation-matrix-pairs-method",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#building-correlation-matrix-pairs-method",
    "title": "Visual Correlation Analysis",
    "section": "1.4 Building Correlation Matrix: pairs() method",
    "text": "1.4 Building Correlation Matrix: pairs() method\nThere are more than one way to build scatterplot matrix with R. In this section, you will learn how to create a scatterplot matrix by using the pairs function of R Graphics.\nBefore you continue to the next step, you should read the syntax description of pairsfunction.\n\n1.4.1 Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\npairs(wine[,1:11])\n\n\n\n\nThe required input of pairs() can be a matrix or data frame. The code chunk used to create the scatterplot matrix is relatively simple. It uses the default pairs function. Columns 2 to 12 of wine dataframe is used to build the scatterplot matrix. The variables are: fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates and alcohol.\n\npairs(wine[,2:12])\n\n\n\n\n\n\n1.4.2 Drawing the lower corner\npairs function of R Graphics provided many customisation arguments. For example, it is a common practice to show either the upper half or lower half of the correlation matrix instead of both. This is because a correlation matrix is symmetric.\nTo show the lower half of the correlation matrix, the upper.panel argument will be used as shown in the code chunk below.\n\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\nSimilarly, you can display the upper half of the correlation matrix by using the code chun below.\n\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n1.4.3 Including with correlation coefficients\nTo show the correlation coefficient of each pair of variables instead of a scatter plot, panel.cor function will be used. This will also show higher correlations in a larger font.\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#visualising-correlation-matrix-ggcormat",
    "title": "Visual Correlation Analysis",
    "section": "1.5 Visualising Correlation Matrix: ggcormat()",
    "text": "1.5 Visualising Correlation Matrix: ggcormat()\nOne of the major limitation of the correlation matrix is that the scatter plots appear very cluttered when the number of observations is relatively large (i.e. more than 500 observations). To over come this problem, Corrgram data visualisation technique suggested by D. J. Murdoch and E. D. Chow (1996) and Friendly, M (2002) and will be used.\nThe are at least three R packages provide function to plot corrgram, they are:\n\ncorrgram\nellipse\ncorrplot\n\nOn top that, some R package like ggstatsplot package also provides functions for building corrgram.\nIn this section, you will learn how to visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n1.5.1 The basic plot\nOn of the advantage of using ggcorrmat() over many other methods to visualise a correlation matrix is it’s ability to provide a comprehensive and yet professional statistical report as shown in the figure below.\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\ncor.vars argument is used to compute the correlation matrix needed to build the corrgram.\nggcorrplot.args argument provide additional (mostly aesthetic) arguments that will be passed to ggcorrplot::ggcorrplot function. The list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\n\nThe sample sub-code chunk can be used to control specific component of the plot such as the font size of the x-axis, y-axis, and the statistical report.\n\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#building-multiple-plots",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#building-multiple-plots",
    "title": "Visual Correlation Analysis",
    "section": "1.6 Building multiple plots",
    "text": "1.6 Building multiple plots\nSince ggstasplot is an extension of ggplot2, it also supports faceting. However the feature is not available in ggcorrmat() but in the grouped_ggcorrmat() of ggstatsplot.\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nto build a facet plot, the only argument needed is grouping.var.\nBehind group_ggcorrmat(), patchwork package is used to create the multiplot. plotgrid.args argument provides a list of additional arguments passed to patchwork::wrap_plots, except for guides argument which is already separately specified earlier.\nLikewise, annotation.args argument is calling plot annotation arguments of patchwork package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#visualising-correlation-matrix-using-corrplot-package",
    "href": "Hands_On_Exercise/Hands_On_Ex05B/Hands_On_Ex05B.html#visualising-correlation-matrix-using-corrplot-package",
    "title": "Visual Correlation Analysis",
    "section": "1.7 Visualising Correlation Matrix using corrplot Package",
    "text": "1.7 Visualising Correlation Matrix using corrplot Package\nIn this hands-on exercise, we will focus on corrplot. However, you are encouraged to explore the other two packages too.\nBefore getting started, you are required to read An Introduction to corrplot Package in order to gain basic understanding of corrplot package.\n\n1.7.1 Getting started with corrplot\nBefore we can plot a corrgram using corrplot(), we need to compute the correlation matrix of wine data frame.\nIn the code chunk below, cor() of R Stats is used to compute the correlation matrix of wine data frame.\n\nwine.cor &lt;- cor(wine[, 1:11])\n\nNext, corrplot() is used to plot the corrgram by using all the default setting as shown in the code chunk below.\n\ncorrplot(wine.cor)\n\n\n\n\nNotice that the default visual object used to plot the corrgram is circle. The default layout of the corrgram is a symmetric matrix. The default colour scheme is diverging blue-red. Blue colours are used to represent pair variables with positive correlation coefficients and red colours are used to represent pair variables with negative correlation coefficients. The intensity of the colour or also know as saturation is used to represent the strength of the correlation coefficient. Darker colours indicate relatively stronger linear relationship between the paired variables. On the other hand, lighter colours indicates relatively weaker linear relationship.\n\n\n1.7.2 Working with visual geometrics\nIn corrplot package, there are seven visual geometrics (parameter method) can be used to encode the attribute values. They are: circle, square, ellipse, number, shade, color and pie. The default is circle. As shown in the previous section, the default visual geometric of corrplot matrix is circle. However, this default setting can be changed by using the method argument as shown in the code chunk below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n1.7.3 Working with layout\ncorrplor() supports three layout types, namely: “full”, “upper” or “lower”. The default is “full” which display full matrix. The default setting can be changed by using the type argument of corrplot().\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\nThe default layout of the corrgram can be further customised. For example, arguments diag and tl.col are used to turn off the diagonal cells and to change the axis text label colour to black colour respectively as shown in the code chunk and figure below.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n1.7.4 Working with mixed layout\nWith corrplot package, it is possible to design corrgram with mixed visual matrix of one half and numerical matrix on the other half. In order to create a coorgram with mixed layout, the corrplot.mixed(), a wrapped function for mixed visualisation style will be used.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nNotice that argument lower and upper are used to define the visualisation method used. In this case ellipse is used to map the lower half of the corrgram and numerical matrix (i.e. number) is used to map the upper half of the corrgram. The argument tl.pos, on the other, is used to specify the placement of the axis label. Lastly, the diag argument is used to specify the glyph on the principal diagonal of the corrgram.\n\n\n1.7.5 Combining corrgram with the significant test\nIn statistical analysis, we are also interested to know which pair of variables their correlation coefficients are statistically significant.\nWith corrplot package, we can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nWe can then use the p.mat argument of corrplot function as shown in the code chunk below. The figure below shows a corrgram combined with the significant test. The corrgram reveals that not all correlation pairs are statistically significant. For example the correlation between total sulfur dioxide and free surfur dioxide is statistically significant at significant level of 0.1 but not the pair between total sulfur dioxide and citric acid.\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n1.7.6 Reorder a corrgram\nMatrix reorder is very important for mining the hiden structure and pattern in a corrgram. By default, the order of attributes of a corrgram is sorted according to the correlation matrix (i.e. “original”). The default setting can be over-write by using the order argument of corrplot(). Currently, corrplot package support four sorting methods, they are:\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\n“AOE”, “FPC”, “hclust”, “alphabet”. More algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n1.7.7 Reordering a correlation matrix using hclust\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html",
    "href": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html",
    "title": "Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#overview",
    "title": "Heatmap for Visualising and Analysing Multivariate Data",
    "section": "",
    "text": "Heatmaps visualise data through variations in colouring. When applied to a tabular format, heatmaps are useful for cross-examining multivariate data, through placing variables in the columns and observation (or records) in rowa and colouring the cells within the table. Heatmaps are good for showing variance across multiple variables, revealing any patterns, displaying whether any variables are similar to each other, and for detecting if any correlations exist in-between them.\nIn this hands-on exercise, you will gain hands-on experience on using R to plot static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#installing-and-launching-r-packages",
    "href": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#installing-and-launching-r-packages",
    "title": "Heatmap for Visualising and Analysing Multivariate Data",
    "section": "1.2 Installing and Launching R Packages",
    "text": "1.2 Installing and Launching R Packages\nBefore you get started, you are required to open a new Quarto document. Keep the default html as the authoring format.\nNext, you will use the code chunk below to install and launch seriation, heatmaply, dendextend and tidyverse in RStudio.\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#importing-and-preparing-the-data-set",
    "href": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#importing-and-preparing-the-data-set",
    "title": "Heatmap for Visualising and Analysing Multivariate Data",
    "section": "1.3 Importing and Preparing The Data Set",
    "text": "1.3 Importing and Preparing The Data Set\nIn this hands-on exercise, the data of World Happines 2018 report will be used. The data set is downloaded from here. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\n\n1.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import WHData-2018.csv into R and parsed it into tibble R data frame format.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nThe output tibbled data frame is called wh.\n\n\n1.3.2 Preparing the data\nNext, we need to change the rows by country name instead of row number by using the code chunk below.\n\nrow.names(wh) &lt;- wh$Country\n\nNotice that the row number has been replaced into the country name.\n\n\n1.3.3 Transforming the data frame into a matrix\nThe data was loaded into a data frame, but it has to be a data matrix to make your heatmap.\nThe code chunk below will be used to transform wh data frame into a data matrix.\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\nNotice that wh_matrix is in R matrix format."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#static-heatmap",
    "href": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#static-heatmap",
    "title": "Heatmap for Visualising and Analysing Multivariate Data",
    "section": "1.4 Static Heatmap",
    "text": "1.4 Static Heatmap\nThere are many R packages and functions can be used to drawing static heatmaps, they are:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nIn this section, you will learn how to plot static heatmaps by using heatmap() of R Stats package.\n\n1.4.1 heatmap() of R Stats\nIn this sub-section, we will plot a heatmap by using heatmap() of Base Stats. The code chunk is given below.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe order of both rows and columns is different compare to the native wh_matrix. This is because heatmap do a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\n\nHere, red cells denotes small values, and red small ones. This heatmap is not really informative. Indeed, the Happiness Score variable have relatively higher values, what makes that the other variables with small values all look the same. Thus, we need to normalize this matrix. This is done using the scale argument. It can be applied to rows or to columns following your needs.\nThe code chunk below normalises the matrix column-wise.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\nNotice that the values are scaled now. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#creating-interactive-heatmap",
    "href": "Hands_On_Exercise/Hands_On_Ex05C/Hands_On_Ex05C.html#creating-interactive-heatmap",
    "title": "Heatmap for Visualising and Analysing Multivariate Data",
    "section": "1.5 Creating Interactive Heatmap",
    "text": "1.5 Creating Interactive Heatmap\nheatmaply is an R package for building interactive cluster heatmap that can be shared online as a stand-alone HTML file. It is designed and maintained by Tal Galili.\nBefore we get started, you should review the Introduction to Heatmaply to have an overall understanding of the features and functions of Heatmaply package. You are also required to have the user manualof the package handy with you for reference purposes.\nIn this section, you will gain hands-on experience on using heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n1.5.1 Working with heatmaply\n\nheatmaply(mtcars)\n\n\n\n\n\nThe code chunk below shows the basic syntax needed to create n interactive heatmap by using heatmaply package.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nDifferent from heatmap(), for heatmaply() the default horizontal dendrogram is placed on the left hand side of the heatmap.\nThe text label of each raw, on the other hand, is placed on the right hand side of the heat map.\nWhen the x-axis marker labels are too long, they will be rotated by 135 degree from the north.\n\n\n\n\n\n1.5.2 Data trasformation\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentilse.\n\n1.5.2.1 Scaling method\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n1.5.2.2 Normalising method\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e., wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n1.5.2.3 Percentising method\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e., wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n1.5.3 Clustering algorithm\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n1.5.4 Manual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n1.5.5 Statistical approach\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n1.5.6 Seriation\nOne of the problems with hierarchical clustering is that it doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings. Take three items A, B and C. If you ignore reflections, there are three possible orderings: ABC, ACB, BAC. If clustering them gives you ((A+B)+C) as a tree, you know that C can’t end up between A and B, but it doesn’t tell you which way to flip the A+B cluster. It doesn’t tell you if the ABC ordering will lead to a clearer-looking heatmap than the BAC ordering.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized. This is related to a restricted version of the travelling salesman problem.\nHere we meet our first seriation algorithm: Optimal Leaf Ordering (OLO). This algorithm starts with the output of an agglomerative clustering algorithm and produces a unique ordering, one that flips the various branches of the dendrogram around so as to minimize the sum of dissimilarities between adjacent leaves. Here is the result of applying Optimal Leaf Ordering to the same clustering result as the heatmap above.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\nThe default options is “OLO” (Optimal leaf ordering) which optimizes the above criterion (in O(n^4)). Another option is “GW” (Gruvaeus and Wainer) which aims for the same goal but uses a potentially faster heuristic.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n1.5.7 Working with colour palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\nIn the code chunk below, the Blues colour palette of rColorBrewer is used.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n1.5.8 The finishing touch\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html",
    "title": "Treemap Visualisation with R",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#overview",
    "title": "Treemap Visualisation with R",
    "section": "",
    "text": "In this hands-on exercise, you will gain hands-on experiences on designing treemap using appropriate R packages. The hands-on exercise consists of three main section. First, you will learn how to manipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package. Then, you will learn how to plot static treemap by using treemap package. In the third section, you will learn how to design interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#installing-and-launching-r-packages",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#installing-and-launching-r-packages",
    "title": "Treemap Visualisation with R",
    "section": "1.2 Installing and Launching R Packages",
    "text": "1.2 Installing and Launching R Packages\nBefore we get started, you are required to check if treemap and tidyverse pacakges have been installed.\n\npacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#data-wrangling",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#data-wrangling",
    "title": "Treemap Visualisation with R",
    "section": "1.3 Data Wrangling",
    "text": "1.3 Data Wrangling\nIn this exercise, REALIS2018.csv data will be used. This dataset provides information of private property transaction records in 2018. The dataset is extracted from REALIS portal (https://spring.ura.gov.sg/lad/ore/login/index.cfm) of Urban Redevelopment Authority (URA).\n\n1.3.1 Importing the data set\nIn the code chunk below, read_csv() of readr is used to import realis2018.csv into R and parsed it into tibble R data.frame format.\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\nThe output tibble data.frame is called realis2018.\n\n\n1.3.2 Data Wrangling and Manipulation\nThe data.frame realis2018 is in trasaction record form, which is highly disaggregated and not appropriate to be used to plot a treemap. In this section, we will perform the following steps to manipulate and prepare a data.frtame that is appropriate for treemap visualisation:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\nTwo key verbs of dplyr package, namely: group_by() and summarize() will be used to perform these steps.\ngroup_by() breaks down a data.frame into specified groups of rows. When you then apply the verbs above on the resulting object they’ll be automatically applied “by group”.\nGrouping affects the verbs as follows:\n\ngrouped select() is the same as ungrouped select(), except that grouping variables are always retained.\ngrouped arrange() is the same as ungrouped; unless you set .by_group = TRUE, in which case it orders first by the grouping variables.\nmutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x). They are described in detail in vignette(“window-functions”).\nsample_n() and sample_frac() sample the specified number/fraction of rows in each group.\nsummarise() computes the summary for each group.\n\nIn our case, group_by() will used together with summarise() to derive the summarised data.frame.\n\n\n1.3.3 Grouped summaries without the Pipe\nThe code chank below shows a typical two lines code approach to perform the steps.\n\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\nNote\n\n\n\nAggregation functions such as sum() and meadian() obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. The argument na.rm = TRUE removes the missing values prior to computation.\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n1.3.4 Grouped summaries with the pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#designing-treemap-with-treemap-package",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#designing-treemap-with-treemap-package",
    "title": "Treemap Visualisation with R",
    "section": "1.4 Designing Treemap with treemap Package",
    "text": "1.4 Designing Treemap with treemap Package\ntreemap package is a R package specially designed to offer great flexibility in drawing treemaps. The core function, namely: treemap() offers at least 43 arguments. In this section, we will only explore the major arguments for designing elegent and yet truthful treemaps.\n\n1.4.1 Designing a static treemap\nIn this section, treemap() of Treemap package is used to plot a treemap showing the distribution of median unit prices and total unit sold of resale condominium by geographic hierarchy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n1.4.2 Using the basic arguments\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because it’s vaues will be used to map the sizes of the rectangles of the treemaps.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\n\n1.4.3 Working with vColor and type arguments\nIn the code chunk below, type argument is define as value.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\n\n1.4.4 Colours in treemap package\nThere are two arguments that determine the mapping to color palettes: mapping and palette. The only difference between “value” and “manual” is the default value for mapping. The “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color. The “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n1.4.5 The “value” type treemap\nThe code chunk below shows a value type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nalthough the colour palette used is RdYlBu but there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\n\n1.4.6 The “manual” type treemap\nThe “manual” type does not interpret the values as the “value” type does. Instead, the value range is mapped linearly to the colour palette.\nThe code chunk below shows a manual type treemap.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\nThe colour scheme used is very copnfusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n1.4.7 Treemap Layout\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n1.4.8 Working with algorithm argument\nThe code chunk below plots a squarified treemap by changing the algorithm argument.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n1.4.9 Using sortID\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#designing-treemap-using-treemapify-package",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#designing-treemap-using-treemapify-package",
    "title": "Treemap Visualisation with R",
    "section": "1.5 Designing Treemap using treemapify Package",
    "text": "1.5 Designing Treemap using treemapify Package\ntreemapify is a R package specially developed to draw treemaps in ggplot2. In this section, you will learn how to designing treemps closely resemble treemaps designing in previous section by using treemapify. Before you getting started, you should read Introduction to “treemapify” its user guide.\n\n1.5.1 Designing a basic treemap\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n1.5.2 Defining hierarchy\nGrouping by Planning Region,\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\nGrouping by Planning Area,\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\nAdding a boundary line,\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands_On_Exercise/Hands_On_Ex05E/Hands_On_Ex05E.html#designing-interactive-treemap-using-d3treer",
    "title": "Treemap Visualisation with R",
    "section": "1.6 Designing Interactive Treemap using d3treeR",
    "text": "1.6 Designing Interactive Treemap using d3treeR\n\n1.6.1 Installing d3treeR package\n\nlibrary(devtools)\ninstall_github(\"timelyportfolio/d3treeR\")\n\n\nlibrary(d3treeR)\n\n\n\n1.6.2 Designing An Interactive Treemap\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html",
    "href": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html",
    "title": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#overview",
    "title": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data. It is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nBy the end of this hands-on exercise, you will gain hands-on experience on:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#installing-and-launching-r-packages",
    "href": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#installing-and-launching-r-packages",
    "title": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "15.2 Installing and Launching R Packages",
    "text": "15.2 Installing and Launching R Packages\nFor this exercise, the GGally, parcoords, parallelPlot and tidyverse packages will be used.\nThe code chunks below are used to install and load the packages in R.\n\npacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#data-preparation",
    "href": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#data-preparation",
    "title": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "15.3 Data Preparation",
    "text": "15.3 Data Preparation\nIn this hands-on exercise, the World Happinees 2018 (http://worldhappiness.report/ed/2018/) data will be used. The data set is download at https://s3.amazonaws.com/happiness-report/2018/WHR2018Chapter2OnlineData.xls. The original data set is in Microsoft Excel format. It has been extracted and saved in csv file called WHData-2018.csv.\nIn the code chunk below, read_csv() of readr package is used to import WHData-2018.csv into R and save it into a tibble data frame object called wh.\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#plotting-static-parallel-coordinates-plot",
    "href": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#plotting-static-parallel-coordinates-plot",
    "title": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "15.4 Plotting Static Parallel Coordinates Plot",
    "text": "15.4 Plotting Static Parallel Coordinates Plot\nIn this section, you will learn how to plot static parallel coordinates plot by using ggparcoord() of GGally package. Before getting started, it is a good practice to read the function description in detail.\n\n15.4.1 Plotting a simple parallel coordinates\nCode chunk below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\nNotice that only two argument namely data and columns is used. Data argument is used to map the data object (i.e. wh) and columns is used to select the columns for preparing the parallel coordinates plot.\n\n\n15.4.2 Plotting a parallel coordinates with boxplot\nThe basic parallel coordinates failed to reveal any meaning understanding of the World Happiness measures. In this section, you will learn how to makeover the plot by using a collection of arguments provided by ggparcoord().\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\n\n15.4.3 Parallel coordinates with facet\nSince ggparcoord() is developed by extending ggplot2 package, we can combination use some of the ggplot2 function when plotting a parallel coordinates plot.\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n15.4.4 Rotating x-axis text label\nTo make the x-axis text label easy to read, let us rotate the labels by 30 degrees. We can rotate axis text labels using theme() function in ggplot2 as shown in the code chunk below.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nTo rotate x-axis text labels, we use axis.text.x as argument to theme() function.\nTo rotate the x-axis text by an angle 30 degree, we specify element_text(angle = 30)\n\n\n\n\n\n15.4.5 Adjusting the rotated x-axis text label\nRotating x-axis text labels to 30 degrees makes the label overlap with the plot and we can avoid this by adjusting the text location using hjust argument to theme’s text element with element_text(). We use axis.text.x as we want to change the look of x-axis text.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands_On_Exercise/Hands_On_Ex05D/Hands_On_Ex05D.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Visual Multivariate Analysis with Parallel Coordinates Plot",
    "section": "15.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods",
    "text": "15.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js. In this section, you will learn how to use functions provided in parallelPlot package to build interactive parallel coordinates plot.\n\n15.5.1 The basic plot\nThe code chunk below plot an interactive parallel coordinates plot by using parallelPlot().\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\nNotice that some of the axis labels are too long. You will learn how to overcome this problem in the next step.\n\n\n15.5.2 Rotate axis label\nIn the code chunk below, rotateTitle argument is used to avoid overlapping axis labels.\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n15.5.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code chunk below.\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n15.5.4 Parallel coordinates plot with histogram\nIn the code chunk below, histoVisibility argument is used to plot histogram along the axis of each variables.\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#learning-outcome",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#importing-libraries",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#importing-libraries",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "1.2 Importing Libraries",
    "text": "1.2 Importing Libraries\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#the-data",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#the-data",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "1.3 The Data",
    "text": "1.3 The Data\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n1.3.1 Importing the data\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n1.3.2 Examining the data structure\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n1.3.3 Data Preparation\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\n\n\n\n\n\nNote\n\n\n\nBeside extracting the necessary data into attacks data frame, mutate() of dplyr package is used to convert wkday and hour fields into factor so they’ll be ordered when plotting"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#building-the-calendar-heatmaps",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#building-the-calendar-heatmaps",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "1.4 Building the Calendar Heatmaps",
    "text": "1.4 Building the Calendar Heatmaps\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\n\n\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#plotting-multiple-calendar-heatmaps",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#plotting-multiple-calendar-heatmaps",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "1.5 Plotting Multiple Calendar Heatmaps",
    "text": "1.5 Plotting Multiple Calendar Heatmaps\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#plotting-cycle-plot",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#plotting-cycle-plot",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "1.6 Plotting Cycle Plot",
    "text": "1.6 Plotting Cycle Plot\nIn this section, you will learn how to plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\n1.6.1 Data Import\nFor the purpose of this hands-on exercise, arrivals_by_air.xlsx will be used.\nThe code chunk below imports arrivals_by_air.xlsx by using read_excel() of readxl package and save it as a tibble data frame called air.\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n1.6.2 Deriving month and year fields\nNext, two new fields called month and year are derived from Month-Year field.\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n1.6.3 Extracting the target country\nNext, the code chunk below is use to extract data for the target country (i.e., Vietnam)\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n1.6.4 Computing year average arrivals by month\nThe code chunk below uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n1.6.5 Plotting the cycle plot\nThe code chunk below is used to plot the cycle plot.\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#plotting-slopegraph",
    "href": "Hands_On_Exercise/Hands_On_Ex06/Hands_On_Ex06.html#plotting-slopegraph",
    "title": "Visualising and Analysing Time-oriented Data",
    "section": "1.7 Plotting Slopegraph",
    "text": "1.7 Plotting Slopegraph\nIn this section you will learn how to plot a slopegraph by using R.\nBefore getting started, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\n1.7.1 Data Import\nImport the rice data set into R environment by using the code chunk below.\n\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\n\n1.7.2 Plotting the slopegraph\nNext, code chunk below will be used to plot a basic slopegraph as shown below.\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")\n\n\n\n\n\n\n\nLearning Points\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html",
    "href": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html",
    "title": "Choropleth Mapping with R",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#overview",
    "title": "Choropleth Mapping with R",
    "section": "",
    "text": "In this in-class exercise, you will gain hands-on experience on using appropriate R methods to plot analytical maps.\n\n\n\nBy the end of this in-class exercise, you will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#getting-started",
    "title": "Choropleth Mapping with R",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\n\n1.2.1 Installing and loading packages\n\npacman::p_load(tmap, tidyverse, sf)\n\n\n\n1.2.2 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#basic-choropleth-mapping",
    "href": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#basic-choropleth-mapping",
    "title": "Choropleth Mapping with R",
    "section": "1.3 Basic Choropleth Mapping",
    "text": "1.3 Basic Choropleth Mapping\n\n1.3.1 Visualising distribution of non-functional water point\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\ntmap_arrange(p2, p1, nrow = 1)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#choropleth-map-for-rates",
    "href": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#choropleth-map-for-rates",
    "title": "Choropleth Mapping with R",
    "section": "1.4 Choropleth Map for Rates",
    "text": "1.4 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space. That means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n1.4.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n1.4.2 Plotting map of rate\nTo plot a choropleth map showing the distribution of percentage functional water point by LGA,\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#extreme-value-maps",
    "href": "Hands_On_Exercise/Hands_On_Ex08C/Hands_On_Ex08C.html#extreme-value-maps",
    "title": "Choropleth Mapping with R",
    "section": "1.5 Extreme Value Maps",
    "text": "1.5 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n1.5.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n1.5.1.1 Data Preparation\nStep 1: Exclude records with NA.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values.\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geomtry field.\n\n\n\n\n1.5.1.2 Why writing functions?\nWriting a function has three big advantages over using copy-and-paste:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\n\nSource: Chapter 19: Functions of R for Data Science.\n\n\n1.5.1.3 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n1.5.1.4 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vnam, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n1.5.1.5 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n1.5.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n1.5.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n1.5.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n1.5.2.3 Test drive the newly created function\nLet’s test the newly created function.\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n1.5.2.4 Boxmap function\nThe code chunk below is an R function to create a box map. - arguments: - vnam: variable name (as character, in quotes) - df: simple features polygon layer - legtitle: legend title - mtitle: map title - mult: multiplier for IQR - returns: - a tmap-element (plots a map)\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html",
    "href": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html",
    "title": "Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#overview",
    "href": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#overview",
    "title": "Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#getting-started",
    "href": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#getting-started",
    "title": "Choropleth Mapping with R",
    "section": "1.2 Getting Started",
    "text": "1.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#importing-data-into-r",
    "href": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#importing-data-into-r",
    "title": "Choropleth Mapping with R",
    "section": "1.3 Importing Data into R",
    "text": "1.3 Importing Data into R\n\n1.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n1.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"Data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/eunicetaam/eunicet0112/ISSS608-VAA/Hands_On_Exercise/Hands_On_Ex08A/Data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n1.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"Data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n1.3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n1.3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:1])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:1])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n1.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\nmpsz_pop2020 &lt;- read_rds(\"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands_On_Exercise/Hands_On_Ex08A/Hands_On_Ex08A.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Choropleth Mapping with R",
    "section": "1.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "1.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n1.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n\n1.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n1.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n1.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\nLearning Points\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n\n\n1.4.2.3 Drawing a choropleth map using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n1.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n1.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n1.4.3.2 Classification Methods of tmap\nClassification takes a large number of observations and group them into data rangers or classes\ntmap has 10 data classification methods:\n\nfixed\nsd\nequal\npretty (default)\nquantile\nkmeans\nhclust\nbclust\nfisher\njenks or natural breaks\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n1.4.3.3 Using Different Number of Classes\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n1.4.3.4 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n1.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n1.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n1.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n1.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n1.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n1.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n1.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n1.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n1.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=FALSE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n1.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n1.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html",
    "title": "Visualising Geospatial Point Data",
    "section": "",
    "text": "Proportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped. In this hands-on exercise, you will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap.\n\n\nBy the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#learning-outcome",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#learning-outcome",
    "title": "Visualising Geospatial Point Data",
    "section": "",
    "text": "By the end of this hands-on exercise, you will acquire the following skills by using appropriate R packages:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#the-data",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#the-data",
    "title": "Visualising Geospatial Point Data",
    "section": "3.1 The data",
    "text": "3.1 The data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#data-import-and-preparation",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#data-import-and-preparation",
    "title": "Visualising Geospatial Point Data",
    "section": "3.2 Data Import and Preparation",
    "text": "3.2 Data Import and Preparation\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() is used to do the job.\n\nlist(sgpools) \n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#creating-a-sf-data-frame-from-an-aspatial-data-frame",
    "title": "Visualising Geospatial Point Data",
    "section": "3.3 Creating a sf data frame from an aspatial data frame",
    "text": "3.3 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;\n\n\nNotice that a new column called geometry has been added into the data frame.\nThe output shows that sgppols_sf is in point feature class. It’s epsg ID is 3414. The bbox provides information of the extend of the geospatial data."
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#it-all-started-with-an-interactive-point-symbol-map",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#it-all-started-with-an-interactive-point-symbol-map",
    "title": "Visualising Geospatial Point Data",
    "section": "4.1 It all started with an interactive point symbol map",
    "text": "4.1 It all started with an interactive point symbol map\nThe code chunks below are used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#lets-make-it-proportional",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#lets-make-it-proportional",
    "title": "Visualising Geospatial Point Data",
    "section": "4.2 Lets make it proportional",
    "text": "4.2 Lets make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#lets-give-it-a-different-colour",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#lets-give-it-a-different-colour",
    "title": "Visualising Geospatial Point Data",
    "section": "4.3 Lets give it a different colour",
    "text": "4.3 Lets give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)"
  },
  {
    "objectID": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#i-have-a-twin-brothers",
    "href": "Hands_On_Exercise/Hands_On_Ex08B/Hands_On_Ex08B.html#i-have-a-twin-brothers",
    "title": "Visualising Geospatial Point Data",
    "section": "4.4 I have a twin brothers :)",
    "text": "4.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\ntmap_mode(\"plot\")"
  }
]